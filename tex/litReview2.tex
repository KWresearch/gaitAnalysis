	%\documentclass[onecolumn,prl,nobalancelastpage,aps,10pt]{revtex4-1}%\documentclass[rmp,preprint]{revtex4-1}\documentclass[11pt]{article} % Try also "scrartcl" or "paper"\linespread{1.35} \usepackage[margin=2.3cm]{geometry}   % to change margins \usepackage{titling,cite,subfig}             % Uncomment both to    \setlength{\droptitle}{0cm}     % change title position \title{%\vspace{-1.5cm}            % Another way to doFeature Descriptors for Gait Analysis From Depth Sensors}\usepackage{graphicx,bm,times,subfig,amsmath,amsfonts,listings,url}\usepackage[page]{appendix}\usepackage{gensymb}\usepackage{color}\usepackage{microtype}%\renewcommand{\thesection}{\arabic{section}}%\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}%\renewcommand{\thesubsubsection}{\thesubsection.\arabic{subsubsection}}\DeclareMathOperator*{\argmin}{arg\,min}% Fix references\makeatletter\renewcommand{\p@subsection}{}\renewcommand{\p@subsubsection}{}\makeatother\usepackage{bibentry}\begin{document}\title{Feature Descriptors for Gait Analysis from Depth Sensors}\author{Ben Crabbe}\date{\today}\maketitle\section{Introduction}\label{sec:intro} %reference for SPHERE?SPHERE - a Sensor Platform for Healthcare in a Residential Environment, is a interdisciplinary research project being undertaken in Bristol which aims to help solve some of the problems currently faced by the healthcare system in the UK. The goal is to design a range of sensors to be fitted in residential environments that facilitate the care and rehabilitation of inhabitants.  %reference for some medical textbook?One function of these environments will be the monitoring and analysis of the resident’s quality of movement or gait. Gait analysis is an important part of current treatments for conditions such as cerebral palsy, Parkinson's disease, neuromuscular disorders and also for patients recovering from strokes or orthopaedic surgery. It can be used in diagnosis, the determination of best treatment, tracking of recovery, and in predicting fall risk~\cite{Perry1992}. %Key areas of the houses, such as the stairs and the sofas will be filmed by RGB-D (colour and depth) imaging cameras,One method of gait analysis consists of visual assessment of patients performing regular movements by a trained physiotherapist~\cite{Whittle1996} producing a quantitative measure on the gait abnormality rating scale~(GARS), or similar~\cite{Cimolin2014,Hamacher2011}, which can be used to predict the likelihood of falls in the future.~\cite{VanSwearingen1996,Wolfson1990} Apart from the fact that it requires people who have difficultly walking to travel to a clinic, there is also an issue of reliability when a complex measurement is performed by humans who are prone to a significant variance between raters, and between ratings as shown in~\cite{Krebs1985}. Various automatic methods of this kind of gait analysis do exist, however they generally require expensive equipment and must be performed in a clinical setting. Researchers at SPHERE have tackled this problem through building an online gait assessment pipeline.~\cite{Paiement} Using footage from RGB-D (colour and depth) imaging cameras (Microsoft Kinect and Asus Xmotion) placed at key areas of the house, such as the~stairs and the~sofas, a model of the subjects movements is extracted in the form of 3D coordinates of joints. This data is then compared using robust statistical models to a trained~model of perfect motion producing a gait abnormality rating for the patient. This system can produce accurate measures of gait abnormality with no effort on the part of the patient. The data can be used in the same manner as that from the sophisticated motion capture performed in gait analysis laboratories. %It can provide continuous assessment of rehabilitation and due to being performed on a frame by frame basis it can also provide instant notifications if a subject has massively abnormal gait which could be indicative of a fall or other episode.An issue limiting the effectiveness of this system is the skeleton extraction stage. At present stock software packages the Microsoft~Kinect~SDK and OpenNI~SDK have been used to extract skeleton data from depth images. Both of these algorithms perform well when the subject is facing the camera and within the optimum range~1-4m, however, outside of these conditions, or when the subject is partially occluded they fail. To allow the system to generate more usable data and to allow more flexible, less intrusive, camera placement the range of acceptable viewing angles must be increased. Therefore the aim of this project is to research and implement a new method for extracting pose information from the depth images. Success will be measured against these objectives: 1) a reimplementation using our own tracking software rather than from the Microsoft/OpenNI SDKs which would protect the system against issues in the future do to changes to the SDKs. 2) Increasing the range of acceptable viewing angles of the current system, even at the expense of some system accuracy would be considered an improvement. 3) A method that could work accurately at all angles would be ideal. Where the accuracy will be measured by comparing overall system performance on the test data (SPHERE-staircase2014~dataset~\cite{Paiement}) using our pose capture method to the original method.This paper will begin by familiarising the reader with depth imaging technology and Microsoft Kinect sensors in section~\ref{sec:Depth}, then in section~\ref{sec:exSys} we will take an overview of the how the existing system for gait quality analysis works. In section~\ref{sec:cnns} we will present our proposed solution which is to use convolutional neural networks.  \section{Depth Imaging}\label{sec:Depth}\begin{figure}\includegraphics*[width=1\linewidth,clip]{kinectHand}\caption{A typical depth image frame from a Kinect v1. From~\cite{Chen2013}		\label{fig:kinectHand}  } \end{figure}Traditional imaging is done in RGB since this is how humans naturally perceive the world. However, RGB~images can be difficult to handle in human pose capture tasks as the variability in human clothing or skin colour can cause problems with background-foreground separation. Also, variability in lighting conditions can cause the same view to appear differently. In depth images each pixel value represents the distance of that point from the camera. Figure~\ref{fig:kinectHand} shows typical depth image. Depth images are unaffected by changes in lighting or human appearance. They provide a 3D map of the scene making background-foreground separation far easier which can often simplify computer vision tasks.\cite{Chen2013}There are three main technologies used to produce depth images: Time~of~flight~(ToF) cameras, Stereo~imaging cameras and Structured~light cameras. It was not until the last~5~years that affordable good~quality RGB-D sensors came on the market, since then there has been an explosion in their use in the computer vision community~\cite{Han2013}. The most prolific of these, and the sensor we plan to use for this project, is the Microsoft Kinect. \subsection{Microsoft Kinect}The Kinect, shown in figure~\ref{fig:kinectCam}, uses structured light technology to compute depth (see~figure~\ref{fig:kinectMeasure}). It consists of an infrared laser emitter, an infrared camera, which together make up the depth sensor, and an RGB camera. An infrared laser is passed through a diffraction grating to produce a known pattern of dots that is projected onto the scene then reflected back and captured by the infrared camera. The measured pattern is compared to a reference pattern produced at a known distance of reflection, which has been stored during the calibration process. The surface of reflection being farther or nearer than the reference surface produces a shift in the pattern which is used to determine the depth value~\cite{Zhang2012a,Khoshelham2012a}.\begin{figure}\centering\subfloat[The Microsoft Kinect v1. From \cite{Zhang2012a}		\label{fig:kinectCam} ]{{\includegraphics*[width=0.45\linewidth,clip]{kinectCamera} }}%\qquad\subfloat[Shows the process by which the Kinect v1 computes depth from triangulation of structured light. From \cite{Han2013}		\label{fig:kinectMeasure} ]{{\includegraphics*[width=0.45\linewidth,clip]{kinectMeasure} }}%\caption{}\label{fig:kinect}\end{figure}\subsubsection{Sensor Performance}\label{sec:senPerf}The RGB camera has 8bit channels and operates at a range of resolutions from 640~$\times$~480 up to~1280~$\times$~1024 with a trade-off in frames-per-second, decreasing from~30 to~10. The range of the depth sensor is generally something like~0.8-3.5m with increasingly noisy or incomplete readings up to 8m. It has a~43$\degree$~vertical by~57$\degree$~horizontal field~of~view and a motorised vertical tilt of~27$\degree$.\cite{Han2013} \begin{figure}\includegraphics*[width=1\linewidth,clip]{kinectHoles4}\caption{Shows the holes in Kinect depth data due to the different perspectives of IR projector and senor (regions~1~and~3) and due to the surface of reflection being roughly~5m away and at a large angle(region~2) From~\cite{Feng2013}		\label{fig:kinectHoles4}  } \end{figure} \begin{figure}\includegraphics*[width=1\linewidth,clip]{kinectHoles3}\caption{Shows the holes in Kinect depth data due abnormal reflections from certain glossy surfaces like the TV monitor and the subjects hair.  From~\cite{Feng2013}			\label{fig:kinectHoles3}  } \end{figure}Stoyanov et al.~\cite{StoyanovTodorandLouloudiAthanasiaandAndreassonHenrikandLilienthal2011a} compare the performance of the Kinect with that of two other ToF depth imaging cameras (SwissRanger~SR-4000 and Fotonic~B70~ToF) assessing them against a ground truth of expensive and low fps laser depth scanner measurements. They find that within a range of 3.5m the Kinect outperforms that of the ToF sensors and is comparable to the laser scanner, and that outside of this range the accuracy falls considerably.Both Khoshelham~\&~Elberink~\cite{Khoshelham2012a} and Smisek~et~al.~\cite{Smisek2011} have measured this effect experimentally comparing Kinect measurements with those from high performance laser scanners. They find temporally fluctuating noise in the depth measurements increases quadratically with distance from the sensor so the depth precision decreases from about~±0.5cm~at~1m to~±7cm at~7m. Nguyen~et~al. shows there to also be linearly increasing noise with lateral distance, and greatly increased noise on surfaces at greater than 70$\degree$~angles~\cite{Nguyen2012}. This last effect can lead to increased levels of noise around edges of humans. As well as noise, the Kinect can often return 'unknown' depth value pixels, known as holes, when the IR receiver cannot read the reflected pattern properly. This can occur around the sides of foreground objects due to the slightly different viewing angles between the projector and camera as in regions 2~and~3 of figure~\ref{fig:kinectHoles4}, or when certain surface materials, such as human hair, interfere with the IR pattern's reflection as in region~4 in figure~\ref{fig:kinectHoles3}. It should be noted that each of these studies mentioned above~\cite{Khoshelham2012a,Smisek2011,Nguyen2012} fail to consider the environmental factors in the quality of the measurement. Fiedler~\&~Muller \cite{Fiedler2013} show that air draft can cause changes of the depth values up to~21mm at a distance of~1.5m, and temperature variations cause changes up to~1.88mm~per~1$\degree$~C. They also find a temperature dependant drift in the position of objects captured by the RGB camera.\subsubsection{Preproccesing}Since we will be working with the depth image directly the random noise and holes in the depth data identified in section~\ref{sec:senPerf} present a problem for our application, for feature extraction to work we need a consistent image. There exists a wide range of papers treating the enhancement of Kinect depth data, each method has some trade off between speed and quality~\cite{Camplani2012,Miao2012,Yang2012a,Chen2013c,Feng2013,Hu2013a,Liu2013,Matsuo2013,Shen2013,Chen2014,Chiu2014,Kim2014,Le2014,Shen2014,Stommel2014}.This will not be a focus of this project as their are other members of SPHERE working on the recognition and segmentation of subjects which are processing the raw images using~\cite{Camplani2012}.\subsubsection{Calibration}Over time the depth values produced by the Kinect can become inaccurate because the calibration between the IR projector and the IR camera becomes invalid. This can be caused by heat or vibration or a drift in the IR laser~\cite{Zhang2012a}. Also for algorithms which combine RGB and depth to work correctly the RGB and Depth maps need to be calibrated. A number of parameters are stored in memory to handle this, including individual parameters for each camera, the rotation and translation between the cameras, and parameters that help determine depth~\cite{Zhang2011c}. The sensor comes with a card which can be shown to the camera periodically to re-calibrate using an adaption of~\cite{Zhang2000a}. There are more precise methods using a checkerboard pattern, and incorporating knowledge of noise patterns identified in section~\ref{sec:senPerf} to improve accuracy~\cite{Zhang2011c,Smisek2011,DanielHerrera2012a}.%Since one of the key design aspects of this system is online analysis of the data, the preprocessing must operate in real time. %Low-cost commodity depth sensor comparison and accuracy analysis.......Low cost depth sensors have been a huge success in the field of computer vision and robotics, providing depth images even in untextured environments. The same characteristic applies to the Kinect V2, a time-of-flight camera with high lateral resolution. In order to assess advantages of the new sensor over its predecessor for standard applications, we provide an analysis of measurement noise, accuracy and other error sources with the Kinect V2. We examined the raw sensor data by using an open source driver. Further insights on the sensor design and examples of processing techniques are given to completely exploit the unrestricted access to the device. © (2014) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.\subsubsection{Software Tools}\label{sec:sofTools}\begin{figure}\centering\includegraphics*[width=0.8\linewidth,clip]{shotton}\caption{ Shows the pipeline of \cite{Blake2011} for joint position estimation. This algorithm is used in the Kinect SDK's. From \cite{Blake2011}		\label{fig:shotton}  } \end{figure}There are a number of software interfaces for the Kinect currently available, the two most common are the Microsoft~SDK\footnote{Available from http://www.microsoft.com/ en-us/kinectforwindows/ as of \today} and the OpenNi~SDK\footnote{Available from http://structure.io/openni as of \today}. The Microsoft~SDK is proprietary and only available for Windows whereas OpenNI is open source and multi-platform. Both of these contain pose estimation/skeleton tracking software. Operating on a frame by frame basis they return the 3D coordinates of the joints, relative to the camera, of any people in the frame. The OpenNi package records~15~joint~positions and the Microsoft package~20. The data can be temporally noisy, with large fluctuations in limb length possible, particularly when relevant joints are occluded, as shown by Obdrzalek~et~al.~\cite{Obdrzalek2012}. By comparing the performance against a marker based motion capture system they also find that the quality of joint estimates decreases gradually up to a~60$\degree$ view~angle, they suggest that for quantitative assessment of movement (with a context of physiotherapy activities for elderly people) a more anthropometric model with fixed limb lengths is required. These skeleton trackers were employed by SPHERE's existing system~\cite{Paiement}, they were found to perform poorly when subjects were captured from a non horizontal perspective e.g. when filmed ascending a stair case from a camera fixed to the ceiling.The algorithm that both these trackers are based on is~\cite{Blake2011}. In this work Shotton et al. generated an extensive, synthetic data set of depth images of humans of many shapes and sizes in highly varied poses sampled from a large motion capture database. Each pixel in the generated images has been labelled with a body part classification. They use this data set to train a random forest decision classifier which uses some very simple features to perform the same per pixel classification on real depth images (figure~\ref{fig:shotton}) which uses a GPU to improve performance allowing it to operate at~$\sim$200~fps on the Xbox~360~GPU. They also implement their training scheme on a huge scale using a parallel multi-core architecture so as to reduce the training time and enable the use of hundreds of thousands of training examples~\cite{Budiu2011}. This huge set of training data makes the system very successful at handling the variety of possible human body shapes and poses. However, each of the training images are captured from the same vertical angle, and this can limit its effectiveness in some situations such as ours. %The algorithm runs per frame%look at ergonomics studies with skeletoniser, is \subsection{Kinect v2}In 2014 Microsoft released the Kinect v2 which uses ToF technology. A strobing infrared light is emitted and a receiver gathers the reflected light, the delay and the known speed of light is used to calculate depth. The RGB image resolution is increased to 1920~$\times$~1080 and operates at~30~fps dropping to~12 in low light conditions. It has increased FoV up to~89$\degree \times 71 \degree$. Depth measurements have a higher fidelity since each pixel corresponds to a separate measurement~\cite{Lun2015}. The noise profile has been measured by \cite{Breuer2014} who finds it similar to v1. Due to its recent release published material on v2 is comparatively rare. We intend use both versions and compare the results. \section{The Existing System for Gait Quality Analysis}\label{sec:exSys}In this section we will cover the existing system (as originally described in~\cite{Paiement} and~\cite{Tao}) highlighting the points to be considered for our work.\begin{figure}\includegraphics*[width=1\linewidth,clip]{exsysData}\caption{Shows a typical sequence from the SPHERE-staircase2014 dataset.  Incorrectly labelled joint positions can be observed in the right hand side of figures c) d) and e) From \cite{Tao}		\label{fig:exsysData}  } \end{figure}As mentioned in section~\ref{sec:intro} the aim of the system is to quantify the quality of movement. This is achieved through comparing the recorded motion to a taught reference model of perfect motion. Although it has been evaluated specifically for gait measurements, the system aims to be widely applicable. Provided with suitable training data demonstrating perfect motion it could be applied to physiotherapy exercises, or in a sports movement optimisation application with little adaption. This has been demonstrated with the system being applied to boxing and sitting-standing motions presented on the SPHERE web page\footnote{www.irc-sphere.ac.uk/work-package-2/movement-quality}.The gait analysis system has been trained using the SPHERE-staircase2014 dataset~\cite{Paiement}. This dataset includes~48 sequences of~12 individuals walking up stairs, captured by a Kinect camera placed at the top of the stairs in a frontal and downward-looking position. It contains three types of abnormal gaits with lower-extremity musculoskeletal conditions, including freezing of gait and using a leading leg, left or right, in going up the stairs. All frames have been manually labelled as normal or abnormal by a qualified physiotherapist. 17 sequences of normal walking from~6 individuals were used for training purposes and~31 sequences from the remaining~6 subjects with both normal and abnormal walking were kept for testing. An example with the skeleton overlaid is shown in~\ref{fig:exsysData}.The system can be considered as a pipeline, as represented in~\ref{fig:sysPipe}. First, it takes the captured RGB-D scene and uses one of the skeleton tracking packages from section~\ref{sec:sofTools} to extract the joint positions of a body in the frame. The skeletons are averaged over a temporal window to smooth noise before being scaled, rotated and translated to normalise the pose.  \begin{figure}\includegraphics*[width=1\linewidth,clip]{sysPipeline}\caption{Shows the stages in the pipeline for gait quality analysis of the existing system. From~\cite{Tao}		\label{fig:sysPipe}  }\end{figure}Next the low-level feature extraction stage builds a feature vector out of the skeleton data to encode the pose. Tao et al. tests and compares a number of viable feature representations for the skeleton data. They find that using a vector of the each joint coordinate concatenated performs best overall~\cite{Tao}. This pose vector is then processed using the non-linear dimensionality reduction method diffusion maps~\cite{Coifman2006}. The aim of this is to remove redundant information from the~$\sim$45 dimensional joint position vector, and process it into a low-dimensional vector containing just the information necessary to characterise the motion. The system is first trained offline to map the training data into this lower dimensional space whilst retaining the intrinsic variability of the data; this mapping is then extended to new data. In this way the system learns an optimal encoding of pose for the motion being trained. It is this representation that we would like to extract directly from the RGB-D images.Next the reduced pose vector~\textbf{Y} is analysed against learnt two models, of instantaneous pose, and of dynamics. The instantaneous model is a probability density function (pdf) built from all the normal poses in the training data set. The normality of a pose is then the likelihood of the of~\textbf{Y} being described by the trained model. Similarly the dynamical model computes a quality based on the likelihood of~\textbf{Y} given the proceeding frames and the model of normal dynamics. Thresholds on these two normality scores are used to classify each frame normal or abnormal.The system is 'online' in that it measures abnormality on a frame-by-frame basis, rather than processing a recorded sequence offline and measuring across the full sequence. This enables it produce more detailed data on the parts of the motion that are deviating from normality, which is a benefit. This too will have to hold for extracting the pose feature. \section{Convolution Neural Networks}\label{sec:cnns}Since the features we wish to extract are abstract and differ based on the motion contained in the training data it is essential that we also adopt a \textit{representation learning} approach to the task. This means that instead of engineering the system by hand, we will build a system that relies on the data to learn the representations that can be used to determine the pose vector. %Such learning methods can be categorised as supervised or un-supervised. Supervised methods rely on labelled data, learning to map the inputs to the labelled outputs. In our system RGB-D footage that has been through the first stages of the pipeline (figure~\ref{fig:sysPipe}) so that it has been assigned a reduced pose vector will be used as the labelled training data. The system could then be trained to develop viewing angle invariance by providing training footage of the same scene captured from multiple cameras. In this way we can get round the view angle restrictions of the SDK's skeleton trackers. Another way this could be done is by using traditional, marker based, motion capture methods rather than the Kinect skeleton extractors. This may be especially useful for capturing motions that the SDK's find particularly difficult e.g. when props such as chairs or balls are involved.\begin{figure}\includegraphics*[width=1\linewidth,clip]{mylenet}\caption{Shows a typical neural network architecture, based on LeNet5~\cite{LeCun1998} \label{fig:mylenet}  } \end{figure}One learning approach that has been used to learn how to visually extract features in this manner are convolutional neural networks (CNNs). CNNs are modelled on the human visual system, a typical architecture is shown in figure~\ref{fig:mylenet}. They consist of a set of filter maps, essentially matrices (or tensors if applying to multiple channels), which are applied repeatedly across the whole image, the resulting images are then processed by non linear function before being recombined based on their spatial proximity. This process is repeated, enabling it to learn representations of progressively more abstract and high level features that are shift and scale invariant. The final layer in the network processes these images into a feature vector. The weights, or pixel values, of these filters are learnt through training data, and will automatically learn the image representations that can be used to deduce the final feature vector. An example of filters learnt for an image recognition task are shown in figure~\ref{fig:exFilt} aligned with the filters is the image which produced the largest response to that filter. Filters in the first layer respond strongly to edges and act on small regions of the input image. Moving through the layers the spatial size of input being convolved increases through the effect of pooling. This leads to progressively higher-level features that incorporate the information from the lower level features throughout the whole image leading to filters that can correctly determine the breed from an image of a dog as seen in the~4th layer of figure~\ref{fig:exFilt}. CNNs have been successfully applied to many areas of computer vision including object recognition~\cite{LeCun2004,Kavukcuoglu2010,Krizhevsky2012,Zeiler2014,Sermanet2013b,Girshick2014,Simonyan2015,Szegedy2014} where CNNs have dominated the past few imageNet challenges~\cite{Russakovsky}, and including 3D object recognition from RGB-D images~\cite{Alexandre2013,Schwarz2015}, face recognition~\cite{Duffner2007,Osadchy2007,Taigman2014}, text recognition~\cite{LeCun1998,Wang2012}, human action recognition~\cite{Baccouche2011a,Le2011,Ji2013,Simonyan2014}, and human pose estimation~\cite{Pfister,Toshev,Jain2014,Jain2013a,Accv2014,Li2014}. \begin{figure}\includegraphics*[width=0.96\linewidth,clip]{Zieler}\caption{Shows the filters in successive layers of Krizhevsky et al.'s object detection CNN~\cite{Krizhevsky2012}. The images are those (or small sections in the early layers) in the dataset which produced the largest response to the filters.	From \cite{Zeiler2014}	\label{fig:exFilt}  } \end{figure}Although their are no examples of CNNs applied to RGB-D pose estimation they have been used by a number of authors to perform joint location on RGB images and videos.Pfister et al.~\cite{Pfister} used a CNN to find the joint positions of arms from RGB video of people signing in BBC clips. Their system accurately predicts the pose without the need for foreground segmentation, and in real-time (100fps on a single GPU). They show that the network implicitly learns constraints about the human kinematic chain, resulting in significantly better constrained pose estimates, with few serious prediction errors even under self occlusions. This, and similar works \cite{Toshev,Jain2013a,Accv2014,Li2014,Tompson,Tompson2014} on joint position regression prove that CNNs are able to extract the information needed to deduce pose including the inference required when a joint is occluded, as shown in the results from~\cite{Tompson} which are reproduced in figure~\ref{fig:Tompson}. With the additional information from the depth channel, and the fact that our intended output pose feature is simpler than locating every joint position we can be confident this is a viable solution. %Additionally, one of these CNNs for pose estimation could be used as the starting point for our network in what is known as transfer learning. This has been shown to boost CNN performance when there is limited of training data available. \cite{Girshick2014,Oquab2014,Schwarz2015,Alexandre2013,Donahue2014,Yosinski2014}\begin{figure}\includegraphics*[width=1\linewidth,clip]{tompson}\centering\caption{ Shows the 2D joint prediction produced by Tompson et al.'s CNN. We observe that it is able to make accurate prediction even when when the joints are completely occluded. From~\cite{Tompson}		\label{fig:Tompson}  } \end{figure}%Osadchy et al. \cite{Osadchy2007} used a CNN to detect and estimate the orientation of faces from RGB images. They used a dataset of images of faces with a manually assigned label of orientation. They trained a CNN to map the images onto a manifold parameterising the facial orientation. The system was able to detect faces at pitch and yaw angles of ±90$\degree$ and ±60$\degree$ respectively to within 15$\degree$ of manual estimates 80\% of the time. Additionally the system was able to run at 5 fps even on a 2.4GHz Pentium 4. \subsection{Architecture}In this section we will describe the components that make up a typical CNN. There are a number of decisions to be made when designing a CNN that can affect its overall performance, these are known as hyper-parameters.Following the formalisation of LeCun et al.~\cite{LeCun1998}, the network can be expressed as \begin{equation}\boldsymbol{Y}_P = F(\boldsymbol{Z}_P,\boldsymbol{W})\end{equation}where~$\boldsymbol{Z}_{P}$ is the~$P$th training image, the output~$\boldsymbol{Y}_{P}$ is the inferred pose vector for this image, and~$\boldsymbol{W}$ are the trainable parameters of the network. For training purposes we define a loss function \begin{equation}E_P = L(\boldsymbol{D}_P, F(\boldsymbol{Z}_P,\boldsymbol{W}))\label{eq:loss}\end{equation}where~$\boldsymbol{D}_P$ is the labelled pose vector associated to the image~$\boldsymbol{Z}_P$. This function measures the error in the network i.e. the difference between the ideal output and the actual output. Training consists of choosing the best values for~$\boldsymbol{W}$ which minimise this error across the training set. However it is possible to overfit for training data. When this occurs the errors for the network on inputs not in the training set, known as the test set, increases. It can be shown~\cite{Seung1992,Vapnik1994} that the difference between the errors on test set and those on the training set is related to both the size of the training set,~$P$, and the capacity,~$c$,  of the network i.e. \begin{equation}E_{test}-E_{training} \propto \frac{c}{P}\label{eq:errorCap}\end{equation}The capacity of the network is essentially the number of parameters being trained, which depends on the number of layers, the dimensions of the input images and the number of channels, the size and number of convolving filters and the way the filters and the pooling are applied. Increasing capacity will also decrease the size of~$\boldsymbol{E}_{training}$, hence there the aim in designing the network is to find the architecture which minimises both~$\boldsymbol{E}_{training}$ and~$\boldsymbol{E}_{test}-\boldsymbol{E}_{training}$ as far as possible~\cite{LeCun1998}.%Another consideration in architecture design is the training speed. Often CNNs are implemented on GPU since the operations are well suited to parallel computation. Often the bottleneck is the size of the memory on your GPU since the number of parameter that must be stored can become very large especially when using either a large input image or a large number of layers.\label{sec:architecture}\subsubsection{Training Strategies and Transfer Learning}A significant decision is whether to adopt, partially at least, an existing trained network. Depending on the capacity training a CNN from scratch can require a very large dataset. It is clear from equation~\ref{eq:errorCap} that by increasing the size of the test set the CNN will perform better when generalised to new images. Toshev~\&~Szegedy~\cite{Toshev}, who used a CNN for 2D human joint location, employed the architecture of Krizhevsky~et~al.~\cite{Krizhevsky2012} (with a linear regression layer added to the end for outputting the joint positions) after they had shown its successful application to object localisation in~\cite{Szegedy2013}. Krizhevsky~et~al.'s architecture was trained on over 15 million categorised images in the ImageNet data set~\cite{Russakovsky} and is provided with a number of CNN software libraries (see section~\ref{sec:softwareLibraries}) making it a practical choice. Whilst all the learned filters (some of which are shown in figure~\ref{fig:exFilt}), particularly the high level ones, may not be very effective for human pose estimation, it has been shown by Yosinski~et~al.~\cite{Yosinski2014} that even features transferred from distant tasks are better than starting from random weights.Pfister~et~al.~\cite{Pfister} also used an architecture designed for use with ImageNet object classification by Sermanet~et~al.~\cite{Sermanet2013b} and examined the performance of their system under a number of training schemes. First they applied the ImageNet trained system in a 'off the shelf' fashion by just adding a final regression layer to output the joint positions. Doing this they actually observed a decreased performance when compared to training the same architecture from scratch. They also tested the pre-trained network after performing some fine-tuning. Again they find this results in a poorer performance than when training from scratch. They suggest these results indicate that the two tasks (image classification and pose estimation) are sufficiently different to require considerably different weights. %Whilst this contradicts the findings in \cite{Yosinski2014,Sharif2014,Donahue2014} it can possibly be due to the fact that theyJain~et~al.~\cite{Jain2014,Jain2013a} do not use transfer learning. Instead they cite the recently produced large datasets e.g. FLIC~\cite{Sapp2013} which contains~5003 images with~10 labelled joint positions produced using Amazon Mechanical Turk, and the use of data augmentations such as flipping. They suggest that such precisely labelled data is far more effective for training CNNs than typical image classification data sets.Li~\&~Chan in their work on 2D joint estimation from RGB\cite{Li2014} use similar methods to Jain~et~al. pre-training their own architecture on a variety of data sets containing~8000 images, before fine tuning on the specific data set they wish to measure performance on.In their work on 3D joint estimation from RGB\cite{Accv2014} they use the Human3.6M data set~\cite{Ionescu2014} combined with data augmentation techniques. Human3.6M, which also contains depth footage, is one of a number of possibly useful dataset we may be able to use for pre-training our network (see section~\ref{sec:datasets}).Another take on transfer learning by Alexandre~\cite{Alexandre2013} deals not with taking an existing network, but with having separate networks for each channel R,~G,~B and Depth and training them in series using the learnt weights of the first channel as a starting point for the second and so on, and then corroborating the results of each for a final output. By comparing various approaches on an object classification task he finds that this method improves both the error rate and the training time. This suggests that training a CNN on all 4 RGB+D channels at once is not the best approach and that we should consider using separate CNNs for each channel instead.To summarise; due to the pre-trained ImageNet networks~\cite{Sermanet2013b,Krizhevsky2012} being included in a number of CNN software libraries and suggestions of their reasonable effectiveness at joint localisation it might be worth trialling these, along with fine-tuning. Before then trying to develop another architecture and attempting to train it from scratch with the use of some available datasets which will be discussed in section~\ref{sec:datasets}.\label{sec:trans}\subsubsection{Datasets}\label{sec:datasets}As mentioned in section~\ref{sec:trans} training a reasonably high capacity CNN requires large a amount of data if we wish to avoid overfitting. Ideally we would train the system each time on a large collection of data of only the motion we are aiming to measure e.g. walking up the stairs. However, it is probably not practical to collect as much data as we may need. The joint estimation papers mentioned in~\ref{sec:trans} that train from scratch use between~1000-8000 test images each with a reasonable difference in pose (i.e. not adjacent frame of a video). A possible alternative is to leverage a suitable publicly available dataset for training purposes, before fine-tuning the system for specific motion we wish to measure as in~\cite{Li2014} and~\cite{Girshick2014}. To use one of these datasets we would need to first process the skeleton data to generate the reduced pose vector as described in section~\ref{sec:exSys}.Human3.6M\footnote{Available at http://vision.imar.ro/human3.6m/description.php  as of \today}~\cite{Ionescu2014,Ionescu2011} contains over~3.6 million different human poses, viewed from~4 different angles by high resolution RGB cameras, and a single angle by a TOF depth sensor. It includes~32 joint positions captured using an accurate marker based human motion capture system. The motions cover a diverse set of everyday scenarios including conversations, eating, greeting, talking on the phone, posing, sitting, smoking, taking photos, waiting and walking in various non-typical scenarios (with a hand in the pocket, talking on the phone, walking a dog, or buying an item). They fully synchronise both the 2D and the 3D data. RGB-D Person Re-identification Dataset\footnote{Available at http://www.iit.it/en/datasets-and-code/datasets/rgbdid.html as of \today}~\cite{Barbosa2012} contains RGB-D footage and skeleton data, captured with a v1~Kinect, of~79 people walking with some viewed from the front and others from behind. Cornell Activity Datasets: CAD-60 \& CAD-120\footnote{Available at http://pr.cs.cornell.edu/humanactivities/data.php as of \today}~\cite{Sung2011} contain a combined~180 RGB-D videos and skeletons captured with Kinect~v1. They record~4 different people performing a number of regular daily activities. Microsoft Research Cambridge-12 Kinect gesture data set\footnote{Available at http://research.microsoft.com/en-us/um/cambridge/projects/msrc12/ as of \today}~\cite{Fothergill2012} consists of~594 sequences and~719,359~frames of~30 people performing~12 gestures, captured with the Kinect~v1, including joint positions from the Microsoft~SDK.Additionally most papers apply some data augmentation techniques to synthetically increase the size of their training sets. Common practises are to flip the image horizontally, and to take a number of random crops from a slightly larger image. These techniques have been shown to improve the network's invariance to reflection and translation respectively. For quantitative comparisons see~\cite{Howard2013}.\subsubsection{Input layer}%. The input layer (that contains the image) should be divisible by 2 many times. Common numbers include 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common ImageNet ConvNets), 384, and 512Input images to the CNN are required to be of fixed dimensions. There are decisions to be made regarding the format to be used. If we wish to leverage an existing, trained CNN e.g.~\cite{Krizhevsky2012,Sermanet2013b} then we are constrained to use the same input format, in this case~$224 \times 224$ pixels with~3 colour channels. It is common for the input size to be divisible by two many times as typical pooling layers act to half the dimensions.Pfister~et~al.~\cite{Pfister} crops and scales images to this fixed size whilst retaining  the proportions and the full expanse of the human being measured. Toshev~\&~Szegedy~\cite{Toshev} do the same then run the image through a further~3 times at larger scales with only part of the image visible to refine joint predictions. They state that due to its fixed input size of~$220 \times 220$, the network has limited capacity to look at detail. They suggest it learns filters capturing pose properties at coarse scale, and that these are necessary to estimate rough pose but insufficient to always precisely localise the body joints. Since we are in fact measuring pose rather than precise joint locations this suggests a small input may not be a big issue. However their will surely be decreasing performance beyond some point. An additional advantage of a small input is that it reduces the capacity of the network, speeding up training and reducing overfitting.The other authors performing joint position estimation~\cite{Jain2014,Jain2013a,Accv2014,Li2014} use a small fixed input in a sliding window method since they first perform object detection on each joint which does not fit well with our needs. Some authors~\cite{Girshick2014,Donahue2014} in object detection tasks advocate a simple warping procedure to fit images into the networks others~\cite{Krizhevsky2012,Sermanet2013b} use down scaling and warping. It is not entirely clear what the effects these different choices can have since they do not address it. In a recent paper He~et~al.~\cite{He2014} present a method they call spatial pyramid pooling which allows a CNN to accept any size input. They add an additional spatial pyramid pooling layer before the final layer of the network which rescales the feature map at that point to fit the fixed size needed by the final, fully connected, layer. In this work they suggest that recognition accuracy can be compromised due to the content/detail loss or distortion caused by resizing/cropping or warping. They find that their method increases scale-invariance and reduces overfitting, boosting accuracy in object recognition. However, in our case it seems we can probably assume a fixed input size, preferably based on the proportions of a typical human standing.Another point to consider for input images is the number of channels. Gupta~et~al.~\cite{Gupta2014} design a CNN for object detection from RGB-D adapting the work of Girshick~et~al.~\cite{Girshick2014} who's system did the same using just RGB. In~\cite{Girshick2014} they used Krizhevsky~et~al.'s~\cite{Krizhevsky2012} network as a pre-trained starting point. In~\cite{Gupta2014} they seek to do the same whilst incorporating a depth channel. They propose encoding the depth image with three channels at each pixel: horizontal disparity, height above ground, and the angle the pixel’s local surface normal makes with the inferred gravity direction. The latter two channels are computed using the algorithms proposed in~\cite{Gupta2013}\footnote{Available at https://github.com/s-gupta/rgbd as of \today} and all channels are linearly scaled to map observed values across the training dataset to the~0 to~255 range. They measure the performance of the system for object detection and segmentation comparing the use of raw depth replicated to produce a~3 channel image, regular~RGB, the depth treated with their encoding which they refer to as~HHA, and with the corroboration of the~RGB and~HHA networks. They gain a~5\% increase in precision using this~HHA encoding rather than duplicated raw depth. Through RGB and~HHA corroboration they achieve a~56\% relative improvement on the state of the art (which uses a non~CNN method call the deformable parts model). These results suggest that, especially if we are adopting a pre-trained existing network, HHA encoding is probably a good method for adding the depth data. They also show that combining predictions of separate RGB and HHA networks leads to better results than produced by either on their own or a single four channel RGB-D network.\begin{figure}\centering\subfloat[]{{\includegraphics*[width=0.43\linewidth,clip]{schwarzDepth} }}%\qquad\subfloat[ ]{{\includegraphics*[width=0.45\linewidth,clip]{schwarz2} }}%\caption{Shows Schwarz~et~al.'s proposed colourisation of depth. From~\cite{Schwarz2015}		\label{fig:schwarzDepth} }\label{fig:schwarz}\end{figure}In similar work Schwarz~et~al.~\cite{Schwarz2015} propose an object centred colourisation scheme for adapting existing networks to depth data, which they suggest is better tailored to classification and orientation estimation tasks. Their technique, which is shown in figure~\ref{fig:schwarz}, attempts to render the object as a 3d mesh using triangulation from the filled depth map, before presenting the object from a normalised viewing angle using a linear interpolation for previously hidden surfaces of the object. It then attempts to infer the 3D object's centre from the mesh, and colours the depth pixels based on their distance from a line through this centre point, normal to its surface. They achieve state of the art results for RGB-D object recognition.%There are some methods developed for human action recognition such as those by Baccouche et al. \cite{Baccouche2011a} and Ji et al. \cite{Ji2013b} which design 3D CNNs to learn spatio-temporal features. These will not be suitable for our system since they process the whole video at once, which would Another decision that applies only to CNN video analysis is whether to exploit temporal information in the input. Pfister~et~al.~\cite{Pfister} experimented with training their CNN to take multiple frames as input for their work locating joints in BBC signers. Through optimisation experiments on the number of frames, and the spacing of those frames, they found that using 3 adjacent frames performed best. Overall they found a modest~1.3\% improvement when using this method. Similarly, Karpathy et al. looked at how best to use temporal information for the task of classifying RGB videos. They experiment with a number of novel architectures, shown in figure~\ref{fig:karp}. Their results show that the slow fusion model performs best. The Slow Fusion model slowly fuses temporal information throughout the network such that higher layers get access to progressively more global information in both spatial and temporal dimensions. This is implemented by extending the connectivity of all convolutional layers in time and carrying out temporal convolutions in addition to spatial convolutions, as in~\cite{Baccouche2011a,Ji2013}. However, they note that it confers only a~$\sim$1\% advantage over the single frame model. Although they suggest that some of the advantage of temporal information may be lost due to the camera motion of footage in their dataset. %jain mocap\begin{figure}\includegraphics*[width=1\linewidth,clip]{karpathyTemp}\caption{Shows the various different architectures evaluated in~\cite{Karpathy2014} for incorporating information from multiple frames of a video. The frames are represented at the bottom. Red, green, blue and yellow boxes indicate convolutional, normalisation, pooling and fully connected layers respectively. From~\cite{Karpathy2014}		\label{fig:karp}  } \end{figure}\subsubsection{Convolution layers}The hyper-parameters for a convolution layer in a CNN are the number of filters, the spatial size of these filters and the stride at which they are applied to the input. %Together these will determine the dimensions of the output volume, and effect  \begin{figure}\includegraphics*[width=1\linewidth,clip]{cnnArch}\caption{Depiction of the stucture of convolutional layers in a CNN.  From~\cite{Ciresan2011}	\label{fig:convs}  } \end{figure}If we restrict our selves to pure 2D convolution, i.e. a single grey-scale image, we can think of an image as a~$N\times N$ (taking a squares for simplicity) matrix of pixel intensity values,~$I_{x,y}$. Similarly the filter, or kernel as it can be known, as a~$f \times f$ matrix of weights, $W_{i,j}$, plus a single bias term,~$b$. Each filter is applied to a subsection of the image, starting from~$x=1$~$y=1$, producing an activation value the first of which is \begin{equation}M_{1,1} = \sigma(\sum_{i=x=1}^{f}\sum_{j=y=1}^{f}I_{x,y} W_{i,j}+b)\label{eq:filterMapActivation}\end{equation}where~$\sigma(.)$ is a non-linear activation function, typically~$max(0,x)$, known as a rectified linear unit~(ReLU) as discussed in~\cite{Krizhevsky2012}. These activation values are stored in a feature map,~$M$, as shown in figure~\ref{fig:convs}. The next element in the feature map is found by moving the starting position of the filter by the stride size,~$s$, and applying it again so that it travels across the input covering every pixel. The size of the produced feature maps depends on the size of the input image, the size of the filter and the size of the stride as \begin{equation}\frac{N-f}{s+1}+1\label{eq:featuremapsize}\end{equation}We can then think of the set of feature maps produced by each filter as the new image, or output volume, with a depth equal to the number of maps in the layer. Obviously, it is required that the value of equation~\ref{eq:featuremapsize} be a integer. Sometimes it is convenient to pad the edges of the input with zeros to achieve this. The width of zero padding can then also be considered a hyper-parameter and equation \ref{eq:featuremapsize} becomes \begin{equation}\frac{N-f+2P}{s+1}+1\label{eq:featuremapsizePadded}\end{equation}where, $P$, is the width of zero padding.Since Krizhevsky et al.'s~\cite{Krizhevsky2012} success using CNNs in the ImageNet challenge in~2012, where he used~$11\times 11 \times 3 $ filters and a stride of~4 in the 1st convolution layer, authors have attempted to improve on his results, experimenting with these hyper parameters. Zeiler~\&~Fergus~\cite{Zeiler2014}, the winners of ImageNet in~2013 examined~\cite{Krizhevsky2012} in detail using a method to visualise the filters and their activations, as was shown in figure~\ref{fig:exFilt}. They found that the 2nd layer visualisations show aliasing artefacts caused by the~4 pixel stride used in the 1st layer convolutions. To improve performance they reduced these filter's sizes from~$11\times 11$ to $7\times 7$ and made the stride~2, rather than~4. The 2014 winners~\cite{Simonyan2015} go even further using a~$3\times 3$ or smaller sized filter with a stride of~1 at every convolution layer in their network. They show that stacking multiple~$3\times 3$ convolution layers on top of each other, three of which has the same spatial extent as a $7\times 7$, produces a better result since it incorporates more non-linearity from the application of three ReLU's rather than one. Additionally it reduces the number of parameters, reducing the risk of overfitting and memory usage. \subsubsection{Pooling Layer}Although it is possible to stack convolutional layers on top of each other they are often followed by a pooling layer (also called subsampling). The idea of pooling is to reduce the spatial size from the previous layer as seen in figure~\ref{fig:pool}. It can also be defined by its spatial size and stride. Operating on each depth slice individually, i.e. each feature map of the previous layer, the pooling window moves across the image taking the values of the elements in the input, conglomerating them using some operation, typically taking the max value as seen in the right hand side of figure~\ref{fig:pool}. Taking the max value, rather than the mean, has been shown to be vastly more effective at building invariances~\cite{Scherer2010}.\begin{figure}\includegraphics*[width=1\linewidth,clip]{pooling}\caption{Shows the effect of pooling layers in a CNN.  From~\cite{KarLects} \label{fig:pool}  } \end{figure}Typically pooling layers use a window size of 2 or 3 and a stride of 2. Larger values can have a destructive effect on the response of later layers. Overlapping pooling windows have been observed to reduce overfitting~\cite{Krizhevsky2012}, and hence may be beneficial in our system. Another method shown to reduce overfitting was presented by Graham~\cite{Graham2015}. He formulates a version of pooling where the size of the window is randomly adjusted to form combinations of~$1\times 1$, $1\times 2$, $2\times 1$ or~$2\times 2$. The idea of this is to preserve the spatial size of the feature maps for longer, since traditional~$2\times 2$ pooling layers half the spatial size each time, thus allowing more convolution layers and deeper networks. This is particularly relevant when the input image is small. His method is shown to improve on the state of the art for CIFAR-10 (another image classification dataset) by~1\%.Recently some authors have claimed that pooling does not always necessarily improve performance, and that it is possible to achieve the same invariances using just convolutional layers that have large overlaps~\cite{Springenberg2015}. However it seems this may only apply if the training dataset is large enough to not worry about overfitting. \subsubsection{Final Layer}CNNs typically contain one or more fully connected layers at the end as in figure~\ref{fig:convs}. Being fully connected means that rather than having a small filter applied repeatedly across the input, a number of filters of the same dimensions as the input are applied to the whole volume. This then outputs a~$1\times 1 \times K$ volume where~$K$ is the number of filters. This is then identical to regular neural networks where each unit in a layer is connected to every unit in the next.In image classification this output vector would be trained to contain class scores and use a softmax function for normalisation. In our case we want to output the pose vector which means we will require~$K$ to be equal to the number dimensions of the pose vector, shown in~\cite{Tao} to be 3. Most networks employ 2 or 3 fully connected layers with the non-final ones applying a larger number of filters. This helps incorporate information across the image, and all the feature maps, building up high level features like those seen in the last layers of figure~\ref{fig:exFilt}~\cite{Zeiler2014}. \subsection{Training}As mentioned in section \ref{sec:architecture} we will measure the performance of our network through a scalar valued loss function. The squared L2 norm with a pre-factor of $1/2$ is commonly used for continuous regression tasks~\cite{Pfister,Li2014,Toshev} so that equation~\ref{eq:loss} becomes \begin{equation}E=\frac{1}{2P} \sum_{(\boldsymbol{Z},\boldsymbol{D})}^{P}\|\boldsymbol{D}-F(\boldsymbol{Z},\boldsymbol{W},\boldsymbol{b})\|^2_2\label{eq:loss}\end{equation}where~$F(\boldsymbol{Z},\boldsymbol{W},\boldsymbol{b})$ is the output pose vector from the final layer of our CNN which depends on the filter weights and biases used in the network and the input image.~$(\boldsymbol{Z},\boldsymbol{D})$ is a training example consisting of an input image and it's labelled pose vector. The training objective is then to optimise the parameters,~$\boldsymbol{W}$, so as to minimise~E over the training set of~$P$ examples. %where, $E$, is the error over the data set indexed by $P$, which consists of image pixel vectors, $\boldsymbol{Z}$, and a labelled pose vector, $\boldsymbol{D}$. $F(.)$ is the CNN's output pose vector %\begin{equation}\argmin_W \sum_{(\boldsymbol{Z},\boldsymbol{D})}^{P}\|\boldsymbol{D}-F(\boldsymbol{Z},\boldsymbol{W})\|^2_2\end{equation}This optimisation is typically carried out using some form of gradient descent where the parameters are updated as\begin{equation}W^{\ell,K}_{x,y,z}=W^{\ell,K}_{x,y,z}-\eta \frac{\partial E}{\partial W^{\ell,K}_{x,y,z}}\label{eq:gradientDescentW}\end{equation}\begin{equation}b^{\ell,K}=b^{\ell,K}-\eta \frac{\partial E}{\partial b^{\ell,K}}\label{eq:gradientDescentB}\end{equation}where~$W^{\ell,K}_{x,y,z}$ is the weight element at position~$x,y,z$ in the~$K$th filter of layer~$\ell$,~$b^{\ell,K}$ is the bias term for that filter and~$\eta$ is the learning rate.For this we need to calculate the derivative of the error with respect to each of the parameters,~$\frac{\partial E}{\partial b^{\ell,K}}$ and $\frac{\partial E}{\partial W^{\ell,K}_{i,j}}$. \subsubsection{Backpropagation}%Backpropagation is an algorithm for efficiently computing the derivatives of the error w.r.t each network parameter needed for gradient descent.Each element of the final output pose vector,~$F_K$, is computed by an individual filter,~$K$, in the final layer,~$\ell$, operating on the entire input volume,~$\boldsymbol I^\ell$, as\begin{equation}F_K=\sigma(\sum_{x,y,z}I^{\ell}_{x,y,z}W^{\ell,K}_{x,y,z}+b^{\ell,K})= \sigma(z^{\ell,K})\end{equation}where~$z^{\ell,K}$ is employed to denote the weighted sum of layer~$\ell$'s input computed by the filter. Then the error w.r.t a single weight in this filter,~$W^{\ell,K}_{x,y,z}$, can be computed using the chain rule to be~\cite{UFLDL}\begin{equation}\frac{\partial E}{\partial W^{\ell,K}_{x,y,z}}= \frac{\partial E}{\partial F_{K}}\frac{\partial F_K}{\partial z^{\ell,K}}\frac{\partial z^{\ell,K}}{\partial W^{\ell,K}_{x,y,z}}=-(D_K-F_K)\sigma'(z^{\ell,K}) (I^{\ell}_{x,y,z})\end{equation}\begin{equation}\frac{\partial E}{\partial b^{\ell,K}}=  \frac{\partial E}{\partial F_{K}}\frac{\partial F_K}{\partial z^{\ell,K}}\frac{\partial z^{\ell,K}}{\partial b^{\ell,K}}=-(D_K-F_K)\sigma'(z^{\ell,K}).1\end{equation}where~$\sigma'()$ is the derivative of the activation function. In the case~$\sigma=max(0,z^{\ell,K})$ then~$\sigma'=0$ when $z^{\ell,K}\le 0$ and  $\sigma'=z^{\ell,K}$ when~$z^{\ell,K}>0$. Since the factor~$ \frac{\partial E}{\partial F_{K}}\frac{\partial F_K}{\partial z^{\ell,K}}$ is common across the entire filter we denote it \begin{equation}\delta^{\ell,K}= \frac{\partial E}{\partial F_{K}}\frac{\partial F_K}{\partial z^{\ell,K}}=-(D_K-F_K)\sigma'(z^{\ell,K})\end{equation}this term is called the backpropagating error and can be thought of as the amount by which a filter is responsible for the overall error~\cite{UFLDL}.Now to compute these terms for the previous layer,~$\ell-1$ (assuming that this is also fully connected to its previous layer, and hence is of~$1 \times 1 \times K'$ dimensions where~$K'$ is the number of filters (units) that are applied (take input from) the previous layer) we note that the output of each of these, rather than producing a straight error as with the final layer, is fed into each of the filters in the following layer. Therefore each filter~$K'$ has a responsibility for the error,~$\delta^{\ell-1,K'}$, which is the weighted average of all the filters,~$K$, in the final layer,~$\ell$, that use it~\cite{Mitchell1997}, i.e. \begin{equation}\delta^{\ell-1,K'}=(\sum_K  W^{\ell,K}_{1,1,K'}\delta^{\ell,K})\sigma'(z^{\ell-1,K'})\end{equation}In this manner the~$\delta$'s for each fully connected layer can be computed from those in the layer above and as with the output layer the error w.r.t the parameters is just \begin{equation}\frac{\partial E}{\partial W^{\ell-1,K'}_{x,y,z}}=\delta^{\ell-1,K'}I^{\ell-1}_{x,y,z}\end{equation}\begin{equation}\frac{\partial E}{\partial b^{\ell-1,K'}}=\delta^{\ell-1,K'}\end{equation}%Combining these equations for each element in $\boldsymbol F$, i.e. across each filter in the layer and using $\odot$ to denote the element-wise, or Hadamard product, gives\begin{equation}\boldsymbol \delta^{\ell}=-(\boldsymbol D-\boldsymbol F)\odot\sigma'( z^{\ell}(\boldsymbol I^{\ell}))\end{equation}for the final layer and\begin{equation}\boldsymbol \delta^{\ell-n}=((\boldsymbol W^\ell)^T\boldsymbol \delta^{\ell-n+1})\odot\sigma'(  z^{\ell-n}(\boldsymbol I^{\ell-n}))\end{equation}for the preceding layers.These equations hold for the fully connected layers only. To compute the~$\delta$'s in a convolution layer preceding a fully connected layer we must follow the same process, except we need the error for each element in the filter map rather than the whole filter~\cite{UFLDL}. %These errors are again the weighted sum of the errors in the following layer, except this time there are 2 possible dimensions to the maps. So for element~$x, y$ in the map produced by filter~$K'$ in layer~$\ell-2$ which is fully connected to~$K$ filters in layer~$\ell-1$~\cite{gibiansky}
\begin{equation}\delta^{\ell-2,K'}_{x,y}=(\sum_{K} W^{\ell-1,K}_{x,y}\delta^{\ell-1,K})\sigma'(z^{\ell-2,K'})\end{equation}$z$ for each element in each map is as in equation~\ref{eq:filterMapActivation}. Therefore to compute~$\frac{\partial E}{\partial W_{i,j}}$ for each weight in the filters we need to sum the~$\delta$'s for each map element multiplied by the element of input they are applied to (which depends on the stride,~$S$)\begin{equation}\frac{\partial E}{\partial W^{\ell-2,K'}_{i,j}}=\sum_x\sum_y\delta^{\ell-2,K'}_{x,y}I_{i+xS,j+yS}\end{equation}The process of propagating the errors through a pooling layer depends on the operation performed. If mean pooling is applied then the errors of that unit are shared between the units being pooled. If max pooling, then all of the error should be given to the one unit which was biggest~\cite{gibiansky}. When using max pooling it is therefore a good idea to record which of the inputs it retained in the forward pass for efficient backpropagation, these are sometimes referred to as switches~\cite{KarLects}.\subsubsection{Gradient Descent}\label{sec:gradientDecsent}The optimisation process of gradient descent as expressed in equations~\ref{eq:gradientDescentB} and~\ref{eq:gradientDescentW} can be thought of as trying to reach the minimum error value on the manifold parameterised by all of the weights and biases. Even when there are  only a few trainable parameters this manifold will have many local minima, saddle points and flat regions as well as the one global minimum we hope to find. Since the gradient will always point in the direction of steepest descent, whether that leads to the global minimum or not there is no guarantee that it will converge to the global minimum~\cite{LeCun1998a}. In general this training process is extremely delicate, there are a number of hyper parameters, the choice of which can impact heavily the final performance of the network and the time it takes to train.One decision is the size of the dataset,~$P$, over which~$E$ (equation~\ref{eq:loss}) and therefore the parameter updates (equations~\ref{eq:gradientDescentB} and~\ref{eq:gradientDescentW}) are computed. Methods which compute updates over the entire training set lead to the most precise convergence since they compute the gradient with the most accuracy possible. However they can be slow since similar examples, which are bound to exist in the training set, are essentially redundant~\cite{LeCun1998a,Bottou2012}.Stochastic gradient descent methods, where a single, randomly selected training example is used for each iteration can actually perform better in practise. Since each update is a noisy sample of the gradient, there is a possibility for an update to carry the descent into another possibly deeper error basin in the manifold, as demonstrated in~\cite{Heskes1993}. Stochastic methods also make the process of finding good hyper parameters empirically less painful since the progress of convergence can be monitored as it runs, rather than having to wait for the full set to be processed~\cite{LeCun1998a}. However, the final convergence distance will always be limited by the noise in the gradient.Although stochastic gradient descent technically refers to using a single data example per update this is rarely used when training speed is a concern since the training process is readily parallelised. Nearly all recent methods use a mini-batch method with the batch size tailored to their particular hardware implementation, typically 128 or 256 examples~\cite{Simonyan2015,Krizhevsky2012,Toshev,Sermanet2013b}.Each full pass through the training dataset is commonly referred to as an epoch. Training typically requires a significant number of epochs for the network to converge. The actual number varies depending on the size of the network and the choice of hyper-parameters. For the large ImageNet trained networks~\cite{Simonyan2015,Krizhevsky2012,Zeiler2014} around 80 epochs were required. In general the decision of when to stop training can be hard to judge since it is possible for the error to fluctuate before decreasing further, this will be covered further in section~\ref{sec:regularisation}.Particularly for stochastic methods, and to a lesser extent mini-batch, the order in which the data is presented can have an effect on the quality of the training. Examples which produce the largest errors will cause the largest change to network weights, hence, training on many similar examples consecutively will lead to smaller updates. If we instead randomise the examples then the network is likely to have to be altered somewhat between seeing similar examples which should make the errors larger and training more effective~\cite{LeCun1998a}. This suggests that we might want to shuffle all the frames of our data sets rather than presenting it with a continuous clip. Another hyper-parameter that has a big effect on the convergence of the training is the learning rate,~$\eta$, that appears in equations~\ref{eq:gradientDescentB} and~\ref{eq:gradientDescentW}. The learning rate effects the step size of each iteration. The effects of choosing a sub optimal learning rate can be illustrated from a simple 1D example presented in figure~\ref{fig:learningRates}. Defining an optimal value of the learning rate~$\eta_{opt}$ as one that would adjust the weight values directly to the minimum. For values less than~$\eta_{opt}$ then we are guaranteed convergence but it could take a long time. We will also get convergence for values~$\eta_{opt} < \eta <  2 \eta_{opt}$, but for values~$\eta > 2 \eta_{opt}$ one step will take it farther from the minimum than it was before so that it will diverge away from the minimum~\cite{LeCun1998a}.\begin{figure}\centering\includegraphics*[width=0.5\linewidth,clip]{learningRates}\caption{Shows the effect of different learning rates $\eta$ on the adjustments made to the model parameters $\omega$ where we aim to minimise the error $E(\omega)$.   From~\cite{LeCun1998a}		\label{fig:learningRates}  } \end{figure}
In general the network can only ever converge to within a distance of the minimum set by the size of the learning rate, and at speed also specified by this learning rate. Therefore a reduction in the learning rate over successive epochs is often employed so as to speed up learning at the beginning and achieve close convergence at the end of the process. Monitoring the total error (equation~\ref{eq:loss}) over the training set after each epoch and decreasing the training rate by a factor of roughly 10 once the error stops decreasing is a common strategy in recent works~\cite{Zeiler2014,Pfister,Simonyan2015,Sermanet2013b}. This process is also advised in~\cite{Bottou2012}. Some 'adaptive' methods such as AdaGrad~\cite{Duchi2011}, RMSprop (unpublished but presented in one of Geoff Hinton's lectures, the slides of which are frequently cited~\cite{hintonLects}), AdaDelta~\cite{Zeiler2012} and AdaM~\cite{Kingma2015} do not require a manually specified global learning rate. Instead they compute a per parameter learning rate based on the size of the gradients it sees. This can have the advantage that each parameter converges at an equal pace, and can avoid the situation where a global learning rate can cause one parameter to diverge due to being on a steep gradient, whilst others on flat gradients take a long time to converge.Most recent works \cite{Zeiler2014,Pfister,Simonyan2015,Sermanet2013b} also employ some form of momentum in the gradient descent equations. With the standard form, equations~\ref{eq:gradientDescentB} and \ref{eq:gradientDescentW}, written in matrix form become\begin{equation}\boldsymbol v_{t+1}=\mu \boldsymbol v_{t} -\eta \nabla E(\boldsymbol W)\label{eq:gradientDescentMomentum}\end{equation}\begin{equation}\boldsymbol W_{t+1}=\boldsymbol W_{t}+\boldsymbol v_{t+1}\label{eq:gradientDescentMomentum2}\end{equation}where $\mu$ is the momentum, typically set around 0.9. This term essentially brings mass into gradient descent. This has the effect of accelerating the gradient descent particularly in situations where the surface is highly non-spherical~\cite{Polyak1964,Sutskever2013a}. Consider an error surface that contains a long narrow valley that slopes steadily to a minimum. In this case the gradient will always point primarily down the valley walls which means the algorithm spends a long time going up and down the walls rather than down the valley. In this situation the momentum term acts to damp the size of the steps along the walls and produce a larger effective learning rate along the floor of valley. A popular version of this momentum adaption is the Nesterov’s accelerated gradient method~\cite{Nesterov1983,Sutskever2013a,Bengio2013}. There are also various methods which make use of the 2nd derivatives of the error surface. Combining knowledge of the current slope (1st derivatives) and the rate at which this slope is changing (the 2nd derivatives) it is possible to make an estimate of the position of the minimum and set individual learning rates for each weight close to~$\eta_{opt}$(see figure \ref{fig:learningRates}). Whilst these methods should reduce the training time, in reality the computation of the matrix of 2nd derivatives~$H_{i,j}\equiv\frac{\partial^2E}{\partial W_iW_j}$, known as the Hessian, will be so slow for any large networks, since it has~$|\boldsymbol W|^2$ elements, that the training time will actually increase~\cite{LeCun1998a}.Some 2nd order methods which don't require a full computation of the Hessian also exist however many of these require full batch gradient descent. One 2nd order method which can be operated stochastically is LeCun's stochastic diagonal Levenberg Marquardt which was used in Osadchy et al's work on face detection~[66]. This algorithm is shown to result in a~$\times 3$ speed up in training time with no significant computational demands~\cite{LeCun1998a}. However it is uncommon to see 2nd order methods employed in the large convolutional networks which have appeared since 2012. Most of these use simpler SGD variants based on momentum~\cite{KarLects}.In reality the specific form of gradient descent we use will probably be decided by what is implemented in the software libraries we choose to use (see section~\ref{sec:softwareLibraries}) the most likely of which is Caffe \cite{Jia2014}. This library comes with three types: 1) a standard stochastic mini-batch gradient descent with a momentum term. 2) the AdaGrad method of Duchi et al.~\cite{Duchi2011} and 3) Nesterov’s accelerated gradient  method~\cite{Nesterov1983,Sutskever2013a}. Since the library implements these for us it should be  straight forward to try each and compare the results.\subsubsection{Data Preproccessing}The performance of 1st order gradient descent methods will usually be improved when the shape of the error surface is more spherical, since this means each parameter will converge at the same speed avoiding the problem where some weights can diverge whilst others converge extremely slowly. A spherical error surface can actually be encouraged by performing some simple transformations to the training data and labelled outputs~\cite{LeCun1998a,Bottou2012}. One of which is shifting the values of each element of the input so that the total mean is zero. So we should calculate the mean value of each pixel across the dataset, then subtract this from each. Although subtracting a single mean value from all pixels is apparently acceptable~\cite{KarLects}. \cite{LeCun1998a} also recommends scaling the inputs so that they are equal in each dimension. In the case of images all pixels are between 0 and 255 anyway with a roughly equal distribution so this is usually unnecessary~\cite{KarLects}.\subsection{Regularisation}\label{sec:regularisation}As discussed in section \ref{sec:architecture} although the network is trained to minimise its error on the test set, we are really aiming to minimise its error on unseen data. This is called increasing the networks ability to generalise. Methods of regularisation aim at improving the networks generalisability without reducing the capacity or increasing the data set. \subsubsection{Early Stopping}\begin{figure}\centering\subfloat[Shows an idealised plot of training set error and testing/validation set error on the y axis against training time on the x axis. \label{fig:earlyStopping1} ]{{\includegraphics*[width=0.45\linewidth,clip]{earlyStopping1} }}%\qquad\subfloat[Shows a real testing/validation error plot with error on the y axis and epoch on the x.]{{\includegraphics*[width=0.45\linewidth,clip]{earlyStopping2} }}%\caption{From \cite{Prechelt2012}		\label{fig:earlyStopping} }\end{figure}As training proceeds we expect the error on the training set to decrease asymptotically to some value whilst the error on the test set should also decrease before overfitting kicks in and it starts to increase as seen in figure~\ref{fig:earlyStopping1}. Early stopping aims to cease training at the point at which the test error is minimised rather than continuing until the training set error is minimised. Typically the generalisability is actually monitored throughout training using a third data set which is referred to as a validation data set. The testing set is saved for the final performance evaluation.In reality the validation set error will not be smooth (see figure \ref{fig:earlyStopping}) and can change dramatically with each update to the weights. Additionally, this curve only represents the performance on a subset of all the possible unseen examples, the precise position of the minima would be different for a different validation set. Therefore choosing good stopping criteria is difficult and typically involves some trade off between training time and generalisability.Prechelt \cite{Prechelt2012} compares various classes of stopping criteria. He recommends that when overfitting may be a serious concern (probably applicable in our case since specific training data is limited) one should use a measure of generalisation defined as \begin{equation}GL(t) = 100 \cdot (\frac{E_{val}(t)}{E_{opt}(t)}-1)\end{equation}where $E_{opt}(t)$ is the minimum measured $E_{val}$ seen in any previous epoch. Then training should be stopped when~$GL(t)$ exceeds some value which should be determined from visually analysing the graph of~$E_{val}$ against epoch where the network has been trained well into overfitting.\subsubsection{Weight Decay}A common regularisation method employed in most CNN papers~\cite{Simonyan2015,Krizhevsky2012,Sermanet2013b} is to add~$ \frac{\lambda}{2} \|W\|^2$ to the loss function (equation \ref{eq:loss} where~$\lambda$ is the weight decay term. This leads to a~$-\lambda \|W\|$ term in equations ~\ref{eq:gradientDescentB} and~\ref{eq:gradientDescentW} which reduces the size of the weights over successive iterations regardless of gradient. This has the effect of penalising large weights and encourages the network to use each of its inputs a little rather than some large ones a lot. This improves generalisability, a massively over trained network will begin to 'remember' the training set by having some filters which respond massively to a single example. Reducing these large weights makes it harder for this to happen.Evaluating choices of the weight decay term~$\lambda$ is a similar problem to that of choosing the stopping point, since we are interested in reducing the validation error, but this error is extremely unstable. One method is a trial and error search. Another, presented in \cite{Rognvaldsson2012} is to first train the network with~$\lambda = 0$ then choose~$\lambda$ based on the value of the weights at the point of early stopping. \subsubsection{Dropout}Developed by Hinton et al. \cite{Hinton2014}, dropout has been used in nearly all of the recent  work with CNNs~\cite{Toshev,Pfister,Krizhevsky2012,Tompson,Li2014,Szegedy2014,Sermanet2013b,Simonyan2015,Zeiler2014}. The idea (figure \ref{fig:dropout}) is that during training you give each weight (filter pixel) a probability of being turned off (for both forward and backwards propagation) on each example. The probability frequently used is 0.5. At testing time you remove this and multiply the outputs of each weight by its dropout probability. In effect this is like training $2^n$ different networks, where n is the number of weights, and corroborating each of their predictions at test time. This improves the networks generalisability since it stops single weights being relied upon heavily, instead the network must learn to use each of its weights a little.%dropout\begin{figure}\centering\includegraphics*[width=0.85\linewidth,clip]{dropout}\caption{Illustrates the effect of dropout reglarisation on fully connected layers.\label{fig:dropout} From \cite{Hinton2014} } \end{figure}Using dropout each parameter update becomes very noisy since each example samples a random architecture and therefore a different error surface. This typically increases the training time by a factor of 2-3. Since these gradients tend to cancel each other out a lot it is recommended that the learning rate is 10-100 times higher than the optimum for a network without dropout. Also using a larger than average momentum term, 0.95-0.99 rather than 0.9, will help to reduce the effect of noise~\cite{Hinton2014}.Thompson et al.~\cite{Tompson} in their work on joint localisation found that applying standard dropout did not produce the performance boost expected. They draw the conclusion that since each pixel in each map is the effect of the same filter applied at slightly different position, they are often roughly equal. Then when one pixel is turned off, the neighbours are still producing roughly the same effect. They propose what they call spatialDropout where instead of each individual pixel being turned off, each entire filter map is randomly turned off. They find that this improves the performance of their system, particularly for the smallest dataset they measure against. One reason they find regular Dropout ineffective is that their architecture is fully convolutional, containing no fully connected layers, therefore the final outputs are more locally correlated than in standard networks.\subsection{Software Libraries}\label{sec:softwareLibraries}Since the success of CNNs in the ImageNet tasks their use has spread widely, including many commercial applications. This has resulted in a number of libraries being developed to aid their implementation. Two that are frequently cited by researchers and are directly geared for CNN development are\begin{itemize}\item{Caffe\footnote{Available from http://caffe.berkeleyvision.org/ as of \today}~\cite{Jia2014} by the Berkeley Vision and Learning Centre (BVLC) is a C++ library with Python and MATLAB wrappers with CUDA GPU integration (although this is not imposed). Includes a large number of models and their trained weights submitted by researchers and other users.}\item{cuda-convnet\footnote{Available from https://code.google.com/p/cuda-convnet2/ as of \today} by Alex Krizhevsky is a C++ implementation which does impose CUDA meaning it requires a Fermi-generation GPU (GTX 4xx, GTX 5xx, or Tesla equivalent).  It can model arbitrary layer connectivity and network depth. Training is done using the back-propagation algorithm.}\end{itemize}There also exist some packages like Theano\footnote{Available from http://www.deeplearning.net/software/theano/ as of \today} (Python) and Torch7\footnote{Available from http://torch.ch/ as of \today} (Lua) which are more general machine learning libraries with optimised linear algebra routines and automatic backpropagation.\section{Conclusion}We have described the gait quality analysis pipeline already developed by SPHERE researchers~\cite{Paiement}, we identified that to improve the system we need to develop a method of mapping RGB-D images of humans directly onto the pose manifold introduced in that work. We propose that a suitable way to do this is through representation learning using convolutional neural networks. We have reviewed much of the recent work in CNNs, presenting information to inform the choice of architectures, image encodings, training data, optimisation methods and regularisation methods. It is difficult to make concrete recommendations in most of these cases since our application, using RGB-D for pose estimation and the use of the manifold representation for pose, is completely novel. We hope that the information presented here will aid future research by providing a comprehensive review of the possible choices in each area which should then be experimented with during the implementation.\bibliographystyle{plain}\bibliography{library,myLib}\end{document}