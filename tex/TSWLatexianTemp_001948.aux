\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Paiement,Tao}
\citation{SPHEREweb}
\citation{Paiement,Tao}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}}
\citation{Paiement}
\citation{Paiement,Tao}
\citation{Paiement,Tao}
\citation{Chen2013}
\citation{Han2013}
\citation{Zhang2012a,Khoshelham2012a}
\citation{Han2013}
\citation{Han2013}
\citation{Gonzalez-Jorge2013}
\citation{Han2013}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Related Work}{5}{section.2}}
\newlabel{sec:bg}{{2}{5}{Background and Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Depth Imaging}{5}{subsection.2.1}}
\newlabel{sec:Depth}{{2.1}{5}{Depth Imaging}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Sensor Performance}{5}{subsubsection.2.1.1}}
\newlabel{sec:senPerf}{{2.1.1}{5}{Sensor Performance}{subsubsection.2.1.1}{}}
\citation{StoyanovTodorandLouloudiAthanasiaandAndreassonHenrikandLilienthal2011a}
\citation{Khoshelham2012a}
\citation{Smisek2011}
\citation{Nguyen2012}
\citation{Feng2013}
\citation{Feng2013}
\citation{Feng2013}
\citation{Feng2013}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The process by which depth is computed from triangulation of structured light. From \cite  {Han2013} \relax }}{6}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:kinectMeasure}{{1}{6}{The process by which depth is computed from triangulation of structured light. From \cite {Han2013} \relax }{figure.caption.1}{}}
\citation{Forsyth2005}
\citation{Moeslund2006,Hen2009,Poppe2007,Sminchisescu2006,Liu2015}
\citation{Poppe2007}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Shows the holes in structured light depth data due to the different perspectives of IR projector and senor (regions\nobreakspace  {}1\nobreakspace  {}and\nobreakspace  {}3) and due to the surface of reflection being roughly\nobreakspace  {}5m away and at a large angle(region\nobreakspace  {}2) From\nobreakspace  {}\cite  {Feng2013}  \relax }}{7}{figure.caption.2}}
\newlabel{fig:kinectHoles4}{{2}{7}{Shows the holes in structured light depth data due to the different perspectives of IR projector and senor (regions~1~and~3) and due to the surface of reflection being roughly~5m away and at a large angle(region~2) From~\cite {Feng2013}  \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Human Pose Estimation}{7}{subsection.2.2}}
\citation{Sminchisescu2003}
\citation{Hen2009}
\citation{Hen2009,Agarwal2006}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Shows the holes in depth data due abnormal reflections from certain glossy surfaces like the TV monitor and the subjects hair. From\nobreakspace  {}\cite  {Feng2013}  \relax }}{8}{figure.caption.3}}
\newlabel{fig:kinectHoles3}{{3}{8}{Shows the holes in depth data due abnormal reflections from certain glossy surfaces like the TV monitor and the subjects hair. From~\cite {Feng2013}  \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Human Pose Estimation Using Dimensionality Reduction}{8}{subsubsection.2.2.1}}
\citation{Brand1999,Elgammal2004}
\citation{Paiement}
\citation{Elgammal2004}
\citation{Poggio1990}
\citation{Elgammal2004}
\citation{Elgammal2004a}
\citation{Elgammal2004}
\citation{Elgammal2004}
\citation{Brand1999}
\citation{Urtasun2005}
\citation{Lawrence2004}
\citation{Tangkuampien2006}
\citation{Rosales2000,Rosales2001}
\citation{Helten2013}
\citation{Han2013,Giovanni}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  The 1D gait manifold produced from LLE dimensionality reduction on images of shilouttes used by Elgammal et al. to simplify joint tracking. From\nobreakspace  {}\cite  {Elgammal2004}.  \relax }}{10}{figure.caption.4}}
\newlabel{fig:elg}{{4}{10}{The 1D gait manifold produced from LLE dimensionality reduction on images of shilouttes used by Elgammal et al. to simplify joint tracking. From~\cite {Elgammal2004}.  \relax }{figure.caption.4}{}}
\citation{Blake2011}
\citation{Blake2011}
\citation{Blake2011}
\citation{Blake2011}
\citation{Blake2011}
\citation{Shotton2013a}
\citation{Blake2011}
\citation{Ye2011,Wei2011,Baak2011,Zhu2008}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Human Pose Estimation From Depth Images}{11}{subsubsection.2.2.2}}
\newlabel{eq:dcf}{{1}{11}{Human Pose Estimation From Depth Images}{equation.2.1}{}}
\citation{Chan2014}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces An example the depth comparison features from equation \ref  {eq:dcf} used in \cite  {Blake2011} to perform per pixel body part classification. The yellow crosses indicate the image pixel $\boldsymbol  {u}$ being classified. The red circles indicate the offset pixels as defined in equation \ref  {eq:dcf}. They use random forests which combine many such features to give a strong discriminative signal. From\nobreakspace  {}\cite  {Blake2011}  \relax }}{12}{figure.caption.5}}
\newlabel{fig:dcf}{{5}{12}{An example the depth comparison features from equation \ref {eq:dcf} used in \cite {Blake2011} to perform per pixel body part classification. The yellow crosses indicate the image pixel $\boldsymbol {u}$ being classified. The red circles indicate the offset pixels as defined in equation \ref {eq:dcf}. They use random forests which combine many such features to give a strong discriminative signal. From~\cite {Blake2011}  \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Convolutional Neural Networks}{12}{subsection.2.3}}
\newlabel{eq:networkF}{{2}{12}{Convolutional Neural Networks}{equation.2.2}{}}
\citation{KarLects}
\citation{KarLects}
\citation{KarLects}
\citation{KarLects}
\citation{Rumelhart1986}
\citation{Crabbe2015}
\citation{Blake2011}
\citation{Chan2014}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Shows a representation of the activations produced following a number of convolution layers, non-linearities (Rectified Linear Units, or ReLUs, are the function $max(0,x)$) and pooling layers. The network, an online demo from \cite  {KarLects}, is classifying images from the CIFAR-10 dataset using the ConvnetJS library\footnotemark .  \relax }}{14}{figure.caption.6}}
\newlabel{fig:convnet}{{6}{14}{Shows a representation of the activations produced following a number of convolution layers, non-linearities (Rectified Linear Units, or ReLUs, are the function $max(0,x)$) and pooling layers. The network, an online demo from \cite {KarLects}, is classifying images from the CIFAR-10 dataset using the ConvnetJS library\protect \footnotemark .  \relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Shows the effect of pooling layers in a CNN. From\nobreakspace  {}\cite  {KarLects}  \relax }}{14}{figure.caption.7}}
\newlabel{fig:pool}{{7}{14}{Shows the effect of pooling layers in a CNN. From~\cite {KarLects}  \relax }{figure.caption.7}{}}
\citation{Krizhevsky2012}
\citation{Russakovsky}
\citation{Seung1992,Vapnik1994}
\citation{LeCun1998}
\citation{Szegedy2014}
\citation{Sharif2014,Donahue2014,Oquab2014,Girshick2014,Yosinski2014}
\citation{Schwarz2015,Alexandre2013}
\newlabel{eq:errorCap}{{3}{15}{Convolutional Neural Networks}{equation.2.3}{}}
\citation{Jain2013a}
\citation{Jain2014}
\citation{Toshev}
\citation{Krizhevsky2012}
\citation{Accv2014}
\citation{Pfister}
\citation{Sermanet2013a}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Human Pose Estimation Using CNNs}{16}{subsubsection.2.3.1}}
\newlabel{sec:HpeCnn}{{2.3.1}{16}{Human Pose Estimation Using CNNs}{subsubsection.2.3.1}{}}
\citation{Belagiannis}
\citation{Paiement}
\citation{Paiement}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{17}{section.3}}
\newlabel{sec:methods}{{3}{17}{Methods}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dataset}{17}{subsection.3.1}}
\newlabel{sec:preprocessing}{{3.1}{17}{Dataset}{subsection.3.1}{}}
\newlabel{fig:sub1}{{8a}{18}{Subfigure 8a}{subfigure.8.1}{}}
\newlabel{sub@fig:sub1}{{(a)}{a}{Subfigure 8a\relax }{subfigure.8.1}{}}
\newlabel{fig:sub2}{{8b}{18}{Subfigure 8b}{subfigure.8.2}{}}
\newlabel{sub@fig:sub2}{{(b)}{b}{Subfigure 8b\relax }{subfigure.8.2}{}}
\newlabel{fig:sub3}{{8c}{18}{Subfigure 8c}{subfigure.8.3}{}}
\newlabel{sub@fig:sub3}{{(c)}{c}{Subfigure 8c\relax }{subfigure.8.3}{}}
\newlabel{fig:sub4}{{8d}{18}{Subfigure 8d}{subfigure.8.4}{}}
\newlabel{sub@fig:sub4}{{(d)}{d}{Subfigure 8d\relax }{subfigure.8.4}{}}
\newlabel{fig:sub5}{{8e}{18}{Subfigure 8e}{subfigure.8.5}{}}
\newlabel{sub@fig:sub5}{{(e)}{e}{Subfigure 8e\relax }{subfigure.8.5}{}}
\newlabel{fig:sub6}{{8f}{18}{Subfigure 8f}{subfigure.8.6}{}}
\newlabel{sub@fig:sub6}{{(f)}{f}{Subfigure 8f\relax }{subfigure.8.6}{}}
\newlabel{fig:sub7}{{8g}{18}{Subfigure 8g}{subfigure.8.7}{}}
\newlabel{sub@fig:sub7}{{(g)}{g}{Subfigure 8g\relax }{subfigure.8.7}{}}
\newlabel{fig:sub8}{{8h}{18}{Subfigure 8h}{subfigure.8.8}{}}
\newlabel{sub@fig:sub8}{{(h)}{h}{Subfigure 8h\relax }{subfigure.8.8}{}}
\newlabel{fig:sub9}{{8i}{18}{Subfigure 8i}{subfigure.8.9}{}}
\newlabel{sub@fig:sub9}{{(i)}{i}{Subfigure 8i\relax }{subfigure.8.9}{}}
\newlabel{fig:sub10}{{8j}{18}{Subfigure 8j}{subfigure.8.10}{}}
\newlabel{sub@fig:sub10}{{(j)}{j}{Subfigure 8j\relax }{subfigure.8.10}{}}
\newlabel{fig:sub11}{{8k}{18}{Subfigure 8k}{subfigure.8.11}{}}
\newlabel{sub@fig:sub11}{{(k)}{k}{Subfigure 8k\relax }{subfigure.8.11}{}}
\newlabel{fig:sub12}{{8l}{18}{Subfigure 8l}{subfigure.8.12}{}}
\newlabel{sub@fig:sub12}{{(l)}{l}{Subfigure 8l\relax }{subfigure.8.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The subjects contained in the SPHERE staircase 2014 dataset. The depth images have been processed following the proceedure detailed in Section \ref  {sec:depthprec}. \relax }}{18}{figure.caption.8}}
\newlabel{fig:subjects}{{8}{18}{The subjects contained in the SPHERE staircase 2014 dataset. The depth images have been processed following the proceedure detailed in Section \ref {sec:depthprec}. \relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Subject 1.  }}}{18}{subfigure.8.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Subject 2.  }}}{18}{subfigure.8.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Subject 3.  }}}{18}{subfigure.8.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Subject 4.  }}}{18}{subfigure.8.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {Subject 5.  }}}{18}{subfigure.8.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Subject 6.  }}}{18}{subfigure.8.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {Subject 7.  }}}{18}{subfigure.8.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {Subject 8.  }}}{18}{subfigure.8.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {Subject 9.  }}}{18}{subfigure.8.9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(j)}{\ignorespaces {Subject 10.  }}}{18}{subfigure.8.10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(k)}{\ignorespaces {Subject 11.  }}}{18}{subfigure.8.11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(l)}{\ignorespaces {Subject 12.  }}}{18}{subfigure.8.12}}
\citation{Coifman2006}
\citation{Paiement}
\citation{Paiement}
\citation{Tao}
\citation{Tao}
\citation{Paiement}
\citation{Tao}
\citation{Paiement}
\citation{Tao}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Data Preprocessing}{19}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Skeleton Preprocessing}{19}{subsubsection.3.2.1}}
\newlabel{sec:skelPreproc}{{3.2.1}{19}{Skeleton Preprocessing}{subsubsection.3.2.1}{}}
\citation{Paiement}
\citation{Paiement}
\citation{Camplani2012a}
\citation{Camplani2012a}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Depth Preprocessing}{20}{subsubsection.3.2.2}}
\newlabel{sec:depthprec}{{3.2.2}{20}{Depth Preprocessing}{subsubsection.3.2.2}{}}
\newlabel{fig:1-6manifoldNoFlips}{{9a}{21}{Subfigure 9a}{subfigure.9.1}{}}
\newlabel{sub@fig:1-6manifoldNoFlips}{{(a)}{a}{Subfigure 9a\relax }{subfigure.9.1}{}}
\newlabel{fig:1-6manifoldWFlips}{{9b}{21}{Subfigure 9b}{subfigure.9.2}{}}
\newlabel{sub@fig:1-6manifoldWFlips}{{(b)}{b}{Subfigure 9b\relax }{subfigure.9.2}{}}
\newlabel{fig:1-12maniWFlips}{{9c}{21}{Subfigure 9c}{subfigure.9.3}{}}
\newlabel{sub@fig:1-12maniWFlips}{{(c)}{c}{Subfigure 9c\relax }{subfigure.9.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Shows the pose manifolds used in this work.\relax }}{21}{figure.caption.9}}
\newlabel{fig:manifolds}{{9}{21}{Shows the pose manifolds used in this work.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {The manifold produced by normal sequences of subjects 1-6. This was the form used in\nobreakspace {}\cite {Paiement}. We connect points from neighbouring frames for better visualisation of the structure.  }}}{21}{subfigure.9.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {The manifold produced by normal sequences of subjects 1-6 including their horizontal flips. This was the form used in this work.}}}{21}{subfigure.9.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {The manifold produced by all sequences including their flips. This manifold was found to produce incorrect normality analysis.  }}}{21}{subfigure.9.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Shows the skeleton to manifold mapping. Normal gait sequences trace paths between the two corners on the right, with the position of maximal knee flex the turning point. The left most corner of the manifold consists of elongated skeletons which tend to be measured when the subject is too close to the sensor. \relax }}{22}{figure.caption.10}}
\newlabel{fig:skelToManifod}{{10}{22}{Shows the skeleton to manifold mapping. Normal gait sequences trace paths between the two corners on the right, with the position of maximal knee flex the turning point. The left most corner of the manifold consists of elongated skeletons which tend to be measured when the subject is too close to the sensor. \relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Shows all manifold points in red. After image pre-processing we retain only those in the blue area. \relax }}{23}{figure.caption.11}}
\newlabel{fig:manifoldAfterImageProcessing}{{11}{23}{Shows all manifold points in red. After image pre-processing we retain only those in the blue area. \relax }{figure.caption.11}{}}
\newlabel{fig:-1.165-0.6517}{{12a}{23}{Subfigure 12a}{subfigure.12.1}{}}
\newlabel{sub@fig:-1.165-0.6517}{{(a)}{a}{Subfigure 12a\relax }{subfigure.12.1}{}}
\newlabel{fig:0.9485,-0.1622}{{12b}{23}{Subfigure 12b}{subfigure.12.2}{}}
\newlabel{sub@fig:0.9485,-0.1622}{{(b)}{b}{Subfigure 12b\relax }{subfigure.12.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Shows skeletons within 0.02 of the same Y[0],Y[1] position but with maximum difference in Y[2]. Red skeletons are minimum Y[2], black are maximum. We find no clear difference between the Skeleton of minimum and maximum Y[2]. \relax }}{23}{figure.caption.12}}
\newlabel{fig:skelsZcomp}{{12}{23}{Shows skeletons within 0.02 of the same Y[0],Y[1] position but with maximum difference in Y[2]. Red skeletons are minimum Y[2], black are maximum. We find no clear difference between the Skeleton of minimum and maximum Y[2]. \relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Difference of 0.4099 }}}{23}{subfigure.12.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Difference of 0.503. }}}{23}{subfigure.12.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Each normal sequence of the first 6 subjects plotted in a different colour. We observe that Y[2] coordinates for each subject tend to remain within a sub-region of full range. This finding promoted the use of subject fine-tuning which improves the accuracy of Y[2] predictions. This effect is analysed in Section \ref  {sec:finetuning} \relax }}{24}{figure.caption.13}}
\newlabel{fig:subjectsOnManifold}{{13}{24}{Each normal sequence of the first 6 subjects plotted in a different colour. We observe that Y[2] coordinates for each subject tend to remain within a sub-region of full range. This finding promoted the use of subject fine-tuning which improves the accuracy of Y[2] predictions. This effect is analysed in Section \ref {sec:finetuning} \relax }{figure.caption.13}{}}
\citation{Camplani2012a}
\citation{Camplani2012a}
\citation{bgslibrary}
\citation{Zivkovic2004}
\citation{Zivkovic2004}
\newlabel{fig:depthWithHoles}{{14a}{25}{Subfigure 14a}{subfigure.14.1}{}}
\newlabel{sub@fig:depthWithHoles}{{(a)}{a}{Subfigure 14a\relax }{subfigure.14.1}{}}
\newlabel{fig:camplaniCleaned}{{14b}{25}{Subfigure 14b}{subfigure.14.2}{}}
\newlabel{sub@fig:camplaniCleaned}{{(b)}{b}{Subfigure 14b\relax }{subfigure.14.2}{}}
\newlabel{fig:quickFilled}{{14c}{25}{Subfigure 14c}{subfigure.14.3}{}}
\newlabel{sub@fig:quickFilled}{{(c)}{c}{Subfigure 14c\relax }{subfigure.14.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces A comparison of hole filling methods. We use the max fill method. \relax }}{25}{figure.caption.14}}
\newlabel{fig:filtering}{{14}{25}{A comparison of hole filling methods. We use the max fill method. \relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Typical image before hole filling.  }}}{25}{subfigure.14.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Filtered and filled using a simplifed version of\nobreakspace {}\cite {Camplani2012a}.  }}}{25}{subfigure.14.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Filled using max fill method.  }}}{25}{subfigure.14.3}}
\citation{Krizhevsky2012}
\citation{Krizhevsky2012}
\citation{LeCun1998a,Bottou2010,LeCun2012}
\newlabel{fig:blackBG}{{15a}{28}{Subfigure 15a}{subfigure.15.1}{}}
\newlabel{sub@fig:blackBG}{{(a)}{a}{Subfigure 15a\relax }{subfigure.15.1}{}}
\newlabel{fig:meanBG}{{15b}{28}{Subfigure 15b}{subfigure.15.2}{}}
\newlabel{sub@fig:meanBG}{{(b)}{b}{Subfigure 15b\relax }{subfigure.15.2}{}}
\newlabel{fig:adjusted}{{15c}{28}{Subfigure 15c}{subfigure.15.3}{}}
\newlabel{sub@fig:adjusted}{{(c)}{c}{Subfigure 15c\relax }{subfigure.15.3}{}}
\newlabel{fig:rgb}{{15d}{28}{Subfigure 15d}{subfigure.15.4}{}}
\newlabel{sub@fig:rgb}{{(d)}{d}{Subfigure 15d\relax }{subfigure.15.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Shows the input image colour schemes tested with the CNN. We found (c) produced the largest responses in the first layer filters of a pre-trained network, shown in figure \ref  {fig:colourschemeActivations}\relax }}{28}{figure.caption.15}}
\newlabel{fig:colourschemes}{{15}{28}{Shows the input image colour schemes tested with the CNN. We found (c) produced the largest responses in the first layer filters of a pre-trained network, shown in figure \ref {fig:colourschemeActivations}\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Black background scheme.  }}}{28}{subfigure.15.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Mean background method.  }}}{28}{subfigure.15.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Final scheme.  }}}{28}{subfigure.15.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Pre-processed RGB  }}}{28}{subfigure.15.4}}
\newlabel{fig:layer1filts}{{16a}{29}{Subfigure 16a}{subfigure.16.1}{}}
\newlabel{sub@fig:layer1filts}{{(a)}{a}{Subfigure 16a\relax }{subfigure.16.1}{}}
\newlabel{fig:blackBGactiv}{{16b}{29}{Subfigure 16b}{subfigure.16.2}{}}
\newlabel{sub@fig:blackBGactiv}{{(b)}{b}{Subfigure 16b\relax }{subfigure.16.2}{}}
\newlabel{fig:meanBGactiv}{{16c}{29}{Subfigure 16c}{subfigure.16.3}{}}
\newlabel{sub@fig:meanBGactiv}{{(c)}{c}{Subfigure 16c\relax }{subfigure.16.3}{}}
\newlabel{fig:adjustedActiv}{{16d}{29}{Subfigure 16d}{subfigure.16.4}{}}
\newlabel{sub@fig:adjustedActiv}{{(d)}{d}{Subfigure 16d\relax }{subfigure.16.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The activations produced by the first layer filters (a) for each of the colour schemes of figure \ref  {fig:colourschemes}. \relax }}{29}{figure.caption.16}}
\newlabel{fig:colourschemeActivations}{{16}{29}{The activations produced by the first layer filters (a) for each of the colour schemes of figure \ref {fig:colourschemes}. \relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {The filters in the 1st convolution layer of a pre-trained then fine-tuned AlexNet \cite {Krizhevsky2012}.  }}}{29}{subfigure.16.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Black background scheme  }}}{29}{subfigure.16.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Mean background method  }}}{29}{subfigure.16.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Final scheme  }}}{29}{subfigure.16.4}}
\citation{Jia2014}
\citation{Chen,Pfister,Girshick2014,Springenberg2015,Schwarz2015,Simonyan2015,Wang2015}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Shows the network accuracy against number of training examples when using mean subtracted data and non mean subtracted data. The accuracy of network predictions was seen to decrease when using mean subtracted data, hence we abandon this common practise.\relax }}{30}{figure.caption.17}}
\newlabel{fig:meanSubComp}{{17}{30}{Shows the network accuracy against number of training examples when using mean subtracted data and non mean subtracted data. The accuracy of network predictions was seen to decrease when using mean subtracted data, hence we abandon this common practise.\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}RGB Pre-processing}{30}{subsubsection.3.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Shows the network accuracy when using the RGB images only, the depth image replicated for each of the 3 channels only, and a combination of the two which consisted of 1 channel of the average of red and green, 1 channel of green and blue averaged and the depth in the final channel. We use depth only for final results. \relax }}{31}{figure.caption.18}}
\newlabel{fig:rgbdcomp}{{18}{31}{Shows the network accuracy when using the RGB images only, the depth image replicated for each of the 3 channels only, and a combination of the two which consisted of 1 channel of the average of red and green, 1 channel of green and blue averaged and the depth in the final channel. We use depth only for final results. \relax }{figure.caption.18}{}}
\citation{Folk2011}
\citation{Krizhevsky2012}
\citation{Krizhevsky2012}
\citation{Krizhevsky2012}
\citation{Krizhevsky2012}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces  The architecture of\nobreakspace  {}\cite  {Krizhevsky2012} is used.\relax }}{32}{figure.caption.19}}
\newlabel{fig:alexNet}{{19}{32}{The architecture of~\cite {Krizhevsky2012} is used.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}CNN Library}{32}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Network Architecture}{32}{subsection.3.4}}
\newlabel{sec:alexNet}{{3.4}{32}{Network Architecture}{subsection.3.4}{}}
\newlabel{fig:conv1}{{20a}{33}{Subfigure 20a}{subfigure.20.1}{}}
\newlabel{sub@fig:conv1}{{(a)}{a}{Subfigure 20a\relax }{subfigure.20.1}{}}
\newlabel{fig:norm1}{{20b}{33}{Subfigure 20b}{subfigure.20.2}{}}
\newlabel{sub@fig:norm1}{{(b)}{b}{Subfigure 20b\relax }{subfigure.20.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Shows the effect of Local Response Normalisation on 1st layer activations. \relax }}{33}{figure.caption.20}}
\newlabel{fig:LRN}{{20}{33}{Shows the effect of Local Response Normalisation on 1st layer activations. \relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {The activations before LRN.  }}}{33}{subfigure.20.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {And after  }}}{33}{subfigure.20.2}}
\newlabel{eq:loss}{{4}{33}{Network Architecture}{equation.3.4}{}}
\citation{Simonyan2015}
\citation{Chatfield2014}
\citation{Krizhevsky2012}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Shows a validation set loss for various pre-training schemes. We find that beginning training from ImageNet pretrained weights throughout the network hinders performance. Using pre-trained weights only in the first two layers produced the best results. We use this strategy for our final tests. \relax }}{34}{figure.caption.21}}
\newlabel{fig:alexNetpre}{{21}{34}{Shows a validation set loss for various pre-training schemes. We find that beginning training from ImageNet pretrained weights throughout the network hinders performance. Using pre-trained weights only in the first two layers produced the best results. We use this strategy for our final tests. \relax }{figure.caption.21}{}}
\citation{KarLects}
\citation{Belagiannis}
\citation{Li2014}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Regression Vs. Classification}{35}{subsubsection.3.4.1}}
\citation{Duchi2011}
\newlabel{fig:wClasses51}{{22a}{36}{Subfigure 22a}{subfigure.22.1}{}}
\newlabel{sub@fig:wClasses51}{{(a)}{a}{Subfigure 22a\relax }{subfigure.22.1}{}}
\newlabel{fig:wClasses500s}{{22b}{36}{Subfigure 22b}{subfigure.22.2}{}}
\newlabel{sub@fig:wClasses500s}{{(b)}{b}{Subfigure 22b\relax }{subfigure.22.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces The top 3 graphs plot the 3 components of the pose vector for the labels (red), and the network prediction (blue) for a classification network. The 4th plot shows the error measured as the distance between the labels and prediction. A mean error of 0.13 for this sequence was achieved using pure regression. \relax }}{36}{figure.caption.22}}
\newlabel{fig:classesRes}{{22}{36}{The top 3 graphs plot the 3 components of the pose vector for the labels (red), and the network prediction (blue) for a classification network. The 4th plot shows the error measured as the distance between the labels and prediction. A mean error of 0.13 for this sequence was achieved using pure regression. \relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {With 51 classes per component. Mean\nobreakspace {}loss\nobreakspace {}=\nobreakspace {}0.6035 std\nobreakspace {}=\nobreakspace {}0.3154.  }}}{36}{subfigure.22.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {With 500 classes per component. Mean\nobreakspace {}loss\nobreakspace {}=\nobreakspace {}0.7141 std\nobreakspace {}=\nobreakspace {}0.3054.  }}}{36}{subfigure.22.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Training Details}{36}{subsection.3.5}}
\citation{Paiement}
\citation{Paiement}
\citation{Paiement}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{37}{section.4}}
\newlabel{sec:results}{{4}{37}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Fine-Tuning}{37}{subsection.4.1}}
\newlabel{sec:finetuning}{{4.1}{37}{Fine-Tuning}{subsection.4.1}{}}
\newlabel{fig:s1}{{23a}{38}{Subfigure 23a}{subfigure.23.1}{}}
\newlabel{sub@fig:s1}{{(a)}{a}{Subfigure 23a\relax }{subfigure.23.1}{}}
\newlabel{fig:s1FT}{{23b}{38}{Subfigure 23b}{subfigure.23.2}{}}
\newlabel{sub@fig:s1FT}{{(b)}{b}{Subfigure 23b\relax }{subfigure.23.2}{}}
\newlabel{fig:s2}{{23c}{38}{Subfigure 23c}{subfigure.23.3}{}}
\newlabel{sub@fig:s2}{{(c)}{c}{Subfigure 23c\relax }{subfigure.23.3}{}}
\newlabel{fig:s2FT}{{23d}{38}{Subfigure 23d}{subfigure.23.4}{}}
\newlabel{sub@fig:s2FT}{{(d)}{d}{Subfigure 23d\relax }{subfigure.23.4}{}}
\newlabel{fig:s3}{{23e}{38}{Subfigure 23e}{subfigure.23.5}{}}
\newlabel{sub@fig:s3}{{(e)}{e}{Subfigure 23e\relax }{subfigure.23.5}{}}
\newlabel{fig:s3FT}{{23f}{38}{Subfigure 23f}{subfigure.23.6}{}}
\newlabel{sub@fig:s3FT}{{(f)}{f}{Subfigure 23f\relax }{subfigure.23.6}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.1207, std\nobreakspace {}=\nobreakspace {}0.1604. }}}{38}{subfigure.23.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.1227 std\nobreakspace {}=\nobreakspace {}0.1497.  }}}{38}{subfigure.23.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.3859, std\nobreakspace {}=\nobreakspace {}0.3678. }}}{38}{subfigure.23.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.2280 std\nobreakspace {}=\nobreakspace {}0.2457.  }}}{38}{subfigure.23.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.1648, std\nobreakspace {}=\nobreakspace {}0.2772. }}}{38}{subfigure.23.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.1465 std\nobreakspace {}=\nobreakspace {}0.2713.  }}}{38}{subfigure.23.6}}
\newlabel{fig:s4}{{22g}{39}{Subfigure 22g}{subfigure.22.7}{}}
\newlabel{sub@fig:s4}{{(g)}{g}{Subfigure 22g\relax }{subfigure.22.7}{}}
\newlabel{fig:s4FT}{{22h}{39}{Subfigure 22h}{subfigure.22.8}{}}
\newlabel{sub@fig:s4FT}{{(h)}{h}{Subfigure 22h\relax }{subfigure.22.8}{}}
\newlabel{fig:s5}{{22i}{39}{Subfigure 22i}{subfigure.22.9}{}}
\newlabel{sub@fig:s5}{{(i)}{i}{Subfigure 22i\relax }{subfigure.22.9}{}}
\newlabel{fig:s5FT}{{22j}{39}{Subfigure 22j}{subfigure.22.10}{}}
\newlabel{sub@fig:s5FT}{{(j)}{j}{Subfigure 22j\relax }{subfigure.22.10}{}}
\newlabel{fig:s6}{{22k}{39}{Subfigure 22k}{subfigure.22.11}{}}
\newlabel{sub@fig:s6}{{(k)}{k}{Subfigure 22k\relax }{subfigure.22.11}{}}
\newlabel{fig:s6FT}{{22l}{39}{Subfigure 22l}{subfigure.22.12}{}}
\newlabel{sub@fig:s6FT}{{(l)}{l}{Subfigure 22l\relax }{subfigure.22.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Top 3 graphs show the 3 components of the pose vector for the labels (red), and the network prediction (blue). The 4th plot shows the error measured as the distance between the labels and predictions. Fine-tuned results are produced by networks which have been trained with spare sequences of the subject being tested. \relax }}{39}{figure.caption.24}}
\newlabel{fig:s1-6}{{21}{39}{Top 3 graphs show the 3 components of the pose vector for the labels (red), and the network prediction (blue). The 4th plot shows the error measured as the distance between the labels and predictions. Fine-tuned results are produced by networks which have been trained with spare sequences of the subject being tested. \relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.1613, std\nobreakspace {}=\nobreakspace {}0.1109. }}}{39}{subfigure.22.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.0852 std\nobreakspace {}=\nobreakspace {}0.1030.  }}}{39}{subfigure.22.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.1640, std\nobreakspace {}=\nobreakspace {}0.1450. }}}{39}{subfigure.22.9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(j)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.0751 std\nobreakspace {}=\nobreakspace {}0.0751.  }}}{39}{subfigure.22.10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(k)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.1952, std\nobreakspace {}=\nobreakspace {}0.1884. }}}{39}{subfigure.22.11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(l)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.0873 std\nobreakspace {}=\nobreakspace {}0.0970.  }}}{39}{subfigure.22.12}}
\newlabel{fig:s7}{{22a}{40}{Subfigure 22a}{subfigure.22.1}{}}
\newlabel{sub@fig:s7}{{(a)}{a}{Subfigure 22a\relax }{subfigure.22.1}{}}
\newlabel{fig:s7FT}{{22b}{40}{Subfigure 22b}{subfigure.22.2}{}}
\newlabel{sub@fig:s7FT}{{(b)}{b}{Subfigure 22b\relax }{subfigure.22.2}{}}
\newlabel{fig:s8}{{22c}{40}{Subfigure 22c}{subfigure.22.3}{}}
\newlabel{sub@fig:s8}{{(c)}{c}{Subfigure 22c\relax }{subfigure.22.3}{}}
\newlabel{fig:8FT}{{22d}{40}{Subfigure 22d}{subfigure.22.4}{}}
\newlabel{sub@fig:8FT}{{(d)}{d}{Subfigure 22d\relax }{subfigure.22.4}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.1208, std\nobreakspace {}=\nobreakspace {}0.1388. }}}{40}{subfigure.22.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.1133 std\nobreakspace {}=\nobreakspace {}0.1439.  }}}{40}{subfigure.22.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.0457, std\nobreakspace {}=\nobreakspace {}0.0720. }}}{40}{subfigure.22.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.0795 std\nobreakspace {}=\nobreakspace {}0.0950.  }}}{40}{subfigure.22.4}}
\newlabel{fig:s9n1}{{21e}{41}{Subfigure 21e}{subfigure.21.5}{}}
\newlabel{sub@fig:s9n1}{{(e)}{e}{Subfigure 21e\relax }{subfigure.21.5}{}}
\newlabel{fig:s9n1FT}{{21f}{41}{Subfigure 21f}{subfigure.21.6}{}}
\newlabel{sub@fig:s9n1FT}{{(f)}{f}{Subfigure 21f\relax }{subfigure.21.6}{}}
\newlabel{fig:s9rl}{{21g}{41}{Subfigure 21g}{subfigure.21.7}{}}
\newlabel{sub@fig:s9rl}{{(g)}{g}{Subfigure 21g\relax }{subfigure.21.7}{}}
\newlabel{fig:s9rlFT}{{21h}{41}{Subfigure 21h}{subfigure.21.8}{}}
\newlabel{sub@fig:s9rlFT}{{(h)}{h}{Subfigure 21h\relax }{subfigure.21.8}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.1110, std\nobreakspace {}=\nobreakspace {}0.0995. }}}{41}{subfigure.21.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.0692 std\nobreakspace {}=\nobreakspace {}0.0852.  }}}{41}{subfigure.21.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.0944, std\nobreakspace {}=\nobreakspace {}0.0875. }}}{41}{subfigure.21.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {mean loss\nobreakspace {}=0.0821 std\nobreakspace {}=\nobreakspace {}0.0917  }}}{41}{subfigure.21.8}}
\newlabel{fig:s10ll}{{20i}{42}{Subfigure 20i}{subfigure.20.9}{}}
\newlabel{sub@fig:s10ll}{{(i)}{i}{Subfigure 20i\relax }{subfigure.20.9}{}}
\newlabel{fig:s10llFT}{{20j}{42}{Subfigure 20j}{subfigure.20.10}{}}
\newlabel{sub@fig:s10llFT}{{(j)}{j}{Subfigure 20j\relax }{subfigure.20.10}{}}
\newlabel{fig:s11n2}{{20k}{42}{Subfigure 20k}{subfigure.20.11}{}}
\newlabel{sub@fig:s11n2}{{(k)}{k}{Subfigure 20k\relax }{subfigure.20.11}{}}
\newlabel{fig:s11n2FT}{{20l}{42}{Subfigure 20l}{subfigure.20.12}{}}
\newlabel{sub@fig:s11n2FT}{{(l)}{l}{Subfigure 20l\relax }{subfigure.20.12}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.1937 std\nobreakspace {}=\nobreakspace {}0.2353. }}}{42}{subfigure.20.9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(j)}{\ignorespaces {mean loss\nobreakspace {}=0.1575 std\nobreakspace {}=\nobreakspace {}0.2448  }}}{42}{subfigure.20.10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(k)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.1337, std\nobreakspace {}=\nobreakspace {}0.1083. }}}{42}{subfigure.20.11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(l)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.0474 std\nobreakspace {}=\nobreakspace {}0.0496  }}}{42}{subfigure.20.12}}
\newlabel{fig:s11sx2}{{19m}{43}{Subfigure 19m}{subfigure.19.13}{}}
\newlabel{sub@fig:s11sx2}{{(m)}{m}{Subfigure 19m\relax }{subfigure.19.13}{}}
\newlabel{fig:s11sx2FT}{{19n}{43}{Subfigure 19n}{subfigure.19.14}{}}
\newlabel{sub@fig:s11sx2FT}{{(n)}{n}{Subfigure 19n\relax }{subfigure.19.14}{}}
\newlabel{fig:s12n1}{{19o}{43}{Subfigure 19o}{subfigure.19.15}{}}
\newlabel{sub@fig:s12n1}{{(o)}{o}{Subfigure 19o\relax }{subfigure.19.15}{}}
\newlabel{fig:s12n1FT}{{19p}{43}{Subfigure 19p}{subfigure.19.16}{}}
\newlabel{sub@fig:s12n1FT}{{(p)}{p}{Subfigure 19p\relax }{subfigure.19.16}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(m)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.1547, std\nobreakspace {}=\nobreakspace {}0.1231. }}}{43}{subfigure.19.13}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(n)}{\ignorespaces {mean loss\nobreakspace {}=0.0867 std\nobreakspace {}=\nobreakspace {}0.0792  }}}{43}{subfigure.19.14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(o)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.1893, std\nobreakspace {}=\nobreakspace {}0.2492. }}}{43}{subfigure.19.15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(p)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.1673 std\nobreakspace {}=\nobreakspace {}0.2368  }}}{43}{subfigure.19.16}}
\newlabel{fig:s12rl}{{18q}{44}{Subfigure 18q}{subfigure.18.17}{}}
\newlabel{sub@fig:s12rl}{{(q)}{q}{Subfigure 18q\relax }{subfigure.18.17}{}}
\newlabel{fig:s12rlFT}{{18r}{44}{Subfigure 18r}{subfigure.18.18}{}}
\newlabel{sub@fig:s12rlFT}{{(r)}{r}{Subfigure 18r\relax }{subfigure.18.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Top 3 graphs show the 3 components of the pose vector for the labels (red), and the network prediction (blue). The 4th plot shows the error measured as the distance between the labels and predictions. Fine-tuned results are produced by networks which have been trained with spare sequences of the subject being tested. 5th and 6th plots show the movement quality measurements of\nobreakspace  {}\cite  {Paiement} when given the network's prediction (blue) and the original results of the labels (red). The magenta dashed line shows the empirically determined abnormality thresholds. \relax }}{44}{figure.caption.29}}
\newlabel{fig:s7-12}{{17}{44}{Top 3 graphs show the 3 components of the pose vector for the labels (red), and the network prediction (blue). The 4th plot shows the error measured as the distance between the labels and predictions. Fine-tuned results are produced by networks which have been trained with spare sequences of the subject being tested. 5th and 6th plots show the movement quality measurements of~\cite {Paiement} when given the network's prediction (blue) and the original results of the labels (red). The magenta dashed line shows the empirically determined abnormality thresholds. \relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(q)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.1611, std\nobreakspace {}=\nobreakspace {}0.1334. }}}{44}{subfigure.18.17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(r)}{\ignorespaces {mean loss\nobreakspace {}=\nobreakspace {}0.1097 std\nobreakspace {}=\nobreakspace {}0.1040  }}}{44}{subfigure.18.18}}
\newlabel{fig:ftS2}{{18a}{45}{Subfigure 18a}{subfigure.18.1}{}}
\newlabel{sub@fig:ftS2}{{(a)}{a}{Subfigure 18a\relax }{subfigure.18.1}{}}
\newlabel{fig:ftS6}{{18b}{45}{Subfigure 18b}{subfigure.18.2}{}}
\newlabel{sub@fig:ftS6}{{(b)}{b}{Subfigure 18b\relax }{subfigure.18.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Shows a plot of the skeleton/label data for each sequence of the subjects in which fine-tuning produced the greatest decrease in the error of the network's predictions. \relax }}{45}{figure.caption.30}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Subject 2. Fine-tuning on Normal 1 and 2 produced a decrease in error of 0.1579 in 3. }}}{45}{subfigure.18.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Subject 6. Fine-tuning on Normal 1 and 2 produced a decrease in error of 0.1079 in 3.  }}}{45}{subfigure.18.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Shows a 3D plot of the skeleton data for Subject 8. Fine-tuning on Normal 1 and 2 produced an increase in error of 0.0338 in the Stop x2 sequence.\relax }}{46}{figure.caption.31}}
\newlabel{fig:ftS8}{{19}{46}{Shows a 3D plot of the skeleton data for Subject 8. Fine-tuning on Normal 1 and 2 produced an increase in error of 0.0338 in the Stop x2 sequence.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Network Errors Vs. Label Errors Vs. Image Errors}{46}{subsection.4.2}}
\newlabel{sec:errs}{{4.2}{46}{Network Errors Vs. Label Errors Vs. Image Errors}{subsection.4.2}{}}
\citation{Blake2011}
\newlabel{fig:ftS1_257Skel}{{20a}{47}{Subfigure 20a}{subfigure.20.1}{}}
\newlabel{sub@fig:ftS1_257Skel}{{(a)}{a}{Subfigure 20a\relax }{subfigure.20.1}{}}
\newlabel{fig:ftS1_257}{{20b}{47}{Subfigure 20b}{subfigure.20.2}{}}
\newlabel{sub@fig:ftS1_257}{{(b)}{b}{Subfigure 20b\relax }{subfigure.20.2}{}}
\newlabel{fig:ftS1_257}{{20c}{47}{Subfigure 20c}{subfigure.20.3}{}}
\newlabel{sub@fig:ftS1_257}{{(c)}{c}{Subfigure 20c\relax }{subfigure.20.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Shows the image label and prediction for frame 257 of subject 1 normal 1. The results (figure \ref  {fig:s1FT}) showed a large error at this frame which is in fact due to a miss-measured skeleton.\relax }}{47}{figure.caption.32}}
\newlabel{fig:ftS1_257skelAndIm}{{20}{47}{Shows the image label and prediction for frame 257 of subject 1 normal 1. The results (figure \ref {fig:s1FT}) showed a large error at this frame which is in fact due to a miss-measured skeleton.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Labelled skeleton. }}}{47}{subfigure.20.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Depth image.  }}}{47}{subfigure.20.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Skeleton nearest to network's prediction.  }}}{47}{subfigure.20.3}}
\newlabel{fig:badSkel1}{{21a}{49}{Subfigure 21a}{subfigure.21.1}{}}
\newlabel{sub@fig:badSkel1}{{(a)}{a}{Subfigure 21a\relax }{subfigure.21.1}{}}
\newlabel{fig:badSkel2}{{21b}{49}{Subfigure 21b}{subfigure.21.2}{}}
\newlabel{sub@fig:badSkel2}{{(b)}{b}{Subfigure 21b\relax }{subfigure.21.2}{}}
\newlabel{fig:badSkel3}{{21c}{49}{Subfigure 21c}{subfigure.21.3}{}}
\newlabel{sub@fig:badSkel3}{{(c)}{c}{Subfigure 21c\relax }{subfigure.21.3}{}}
\newlabel{fig:badSkel4}{{21d}{49}{Subfigure 21d}{subfigure.21.4}{}}
\newlabel{sub@fig:badSkel4}{{(d)}{d}{Subfigure 21d\relax }{subfigure.21.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Out of range skeleton errors: shows labelled skeletons in red and the skeletons nearest to the network's predicted pose vector in blue for each of the images. These are some of the tested images which produced the greatest errors, occurring at the beginning of sequences where the subject is outside of the Kinect’s optimum range. We find that these errors can be attributed to poor ground truth with the network able to produce a more accurate measure provided that similar poses with correct labels do exist in the training data. \relax }}{49}{figure.caption.33}}
\newlabel{fig:badSkels}{{21}{49}{Out of range skeleton errors: shows labelled skeletons in red and the skeletons nearest to the network's predicted pose vector in blue for each of the images. These are some of the tested images which produced the greatest errors, occurring at the beginning of sequences where the subject is outside of the Kinect’s optimum range. We find that these errors can be attributed to poor ground truth with the network able to produce a more accurate measure provided that similar poses with correct labels do exist in the training data. \relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Subject 2 Normal 3 frame 117 loss = 1.13. Labelled skeleton has legs at a similiar depth, network's prediction has right leg far in front of left. }}}{49}{subfigure.21.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Subject 3 Normal 1 frame 162 loss = 1.30. Labelled skeleton has right leg in front of left, network measures the opposite which is correct. }}}{49}{subfigure.21.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Subject 7 RL frame 75 loss = 0.97. Labelled skeleton is not accurate. The network also fails since this is a strange pose where the subject appears to be turning on to the stairs. Since there are few examples of poses of this kind in the training data, and the ones that are have bad skeletons we can not expect better performance.  }}}{49}{subfigure.21.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Subject 10 LL frame 165 loss = 2.63. Labelled skeleton legs crossed over right above left, network measures left leg forward correctly. }}}{49}{subfigure.21.4}}
\newlabel{fig:skelErr1}{{22a}{50}{Subfigure 22a}{subfigure.22.1}{}}
\newlabel{sub@fig:skelErr1}{{(a)}{a}{Subfigure 22a\relax }{subfigure.22.1}{}}
\newlabel{fig:skelErr2}{{22b}{50}{Subfigure 22b}{subfigure.22.2}{}}
\newlabel{sub@fig:skelErr2}{{(b)}{b}{Subfigure 22b\relax }{subfigure.22.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Skeleton errors: shows for each image the labelled skeleton and corresponding pose vector circled in red, and the network's prediction in blue. \relax }}{50}{figure.caption.34}}
\newlabel{fig:skelErrs}{{22}{50}{Skeleton errors: shows for each image the labelled skeleton and corresponding pose vector circled in red, and the network's prediction in blue. \relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Subject 2 Normal 3 frame 275. Loss = 0.5178. The skeleton of this frame is determined to be inaccurate since relative leg positions disagree considerably with the depth image.  }}}{50}{subfigure.22.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Subject 4 Normal 1 frame 205. Loss = 0.5738. The skeleton of this frame is determined to be inaccurate since there is a clear error in the left foot position being measured at the same position as the right due to occlusion.  }}}{50}{subfigure.22.2}}
\newlabel{fig:netErr1}{{23a}{51}{Subfigure 23a}{subfigure.23.1}{}}
\newlabel{sub@fig:netErr1}{{(a)}{a}{Subfigure 23a\relax }{subfigure.23.1}{}}
\newlabel{fig:netErr2}{{23b}{51}{Subfigure 23b}{subfigure.23.2}{}}
\newlabel{sub@fig:netErr2}{{(b)}{b}{Subfigure 23b\relax }{subfigure.23.2}{}}
\newlabel{fig:netErr3}{{23c}{51}{Subfigure 23c}{subfigure.23.3}{}}
\newlabel{sub@fig:netErr3}{{(c)}{c}{Subfigure 23c\relax }{subfigure.23.3}{}}
\newlabel{fig:netErr4}{{23d}{51}{Subfigure 23d}{subfigure.23.4}{}}
\newlabel{sub@fig:netErr4}{{(d)}{d}{Subfigure 23d\relax }{subfigure.23.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Network Errors: shows label skeleton and corresponding pose vector in red and the network's predicted pose vector in blue for each of the images. In these cases the Kinect skeletons are deemed accurate. By design, the network predicts a pose vector typical of images similar to those it has seen in training. Network errors are caused by under represented pose vectors as in (a), or by inconsistencies with the training data as in (b) and (c).\relax }}{51}{figure.caption.35}}
\newlabel{fig:netErrs}{{23}{51}{Network Errors: shows label skeleton and corresponding pose vector in red and the network's predicted pose vector in blue for each of the images. In these cases the Kinect skeletons are deemed accurate. By design, the network predicts a pose vector typical of images similar to those it has seen in training. Network errors are caused by under represented pose vectors as in (a), or by inconsistencies with the training data as in (b) and (c).\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Subject 2 Normal 3 frame 143 loss = 0.4268. The cause of errors in this sequence is discussed in section \ref {sec:finetuning}.  }}}{51}{subfigure.23.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Subject 12 RL frame 167 loss = 0.3274.  }}}{51}{subfigure.23.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Subject 12 Normal 1 frame 253 loss = 0.3105.  }}}{51}{subfigure.23.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Subject 11 Stop x2 frame 311 loss = 0.2779.  }}}{51}{subfigure.23.4}}
\citation{Paiement}
\citation{Paiement}
\newlabel{fig:badIm1}{{24a}{52}{Subfigure 24a}{subfigure.24.1}{}}
\newlabel{sub@fig:badIm1}{{(a)}{a}{Subfigure 24a\relax }{subfigure.24.1}{}}
\newlabel{fig:badIm2}{{24b}{52}{Subfigure 24b}{subfigure.24.2}{}}
\newlabel{sub@fig:badIm2}{{(b)}{b}{Subfigure 24b\relax }{subfigure.24.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Image Errors: shows labelled skeletons in red and the skeletons nearest to the network's predicted pose vector in blue for each of the images. These are some of the tested images which produced the greatest errors. These errors can be attributed to images that were meant to be removed during pre-processing, either because of bad masks or because the subject was too close to the camera or they were not fully turned towards the camera.\relax }}{52}{figure.caption.36}}
\newlabel{fig:badImages}{{24}{52}{Image Errors: shows labelled skeletons in red and the skeletons nearest to the network's predicted pose vector in blue for each of the images. These are some of the tested images which produced the greatest errors. These errors can be attributed to images that were meant to be removed during pre-processing, either because of bad masks or because the subject was too close to the camera or they were not fully turned towards the camera.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Subject 12 Normal 1 frame 239 loss = 1.06. A poor foreground mask slipped through the cuts due to its roughtly human shape and size.  }}}{52}{subfigure.24.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Subject 10 LL frame 444 loss = 0.77. Subject is too close to the sensor; legs are not visible.  }}}{52}{subfigure.24.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Movement Quality}{52}{subsection.4.3}}
\newlabel{fig:s1err}{{25a}{53}{Subfigure 25a}{subfigure.25.1}{}}
\newlabel{sub@fig:s1err}{{(a)}{a}{Subfigure 25a\relax }{subfigure.25.1}{}}
\newlabel{fig:s2err}{{25b}{53}{Subfigure 25b}{subfigure.25.2}{}}
\newlabel{sub@fig:s2err}{{(b)}{b}{Subfigure 25b\relax }{subfigure.25.2}{}}
\newlabel{fig:s3err}{{25c}{53}{Subfigure 25c}{subfigure.25.3}{}}
\newlabel{sub@fig:s3err}{{(c)}{c}{Subfigure 25c\relax }{subfigure.25.3}{}}
\newlabel{fig:s4err}{{25d}{53}{Subfigure 25d}{subfigure.25.4}{}}
\newlabel{sub@fig:s4err}{{(d)}{d}{Subfigure 25d\relax }{subfigure.25.4}{}}
\newlabel{fig:s5err}{{25e}{53}{Subfigure 25e}{subfigure.25.5}{}}
\newlabel{sub@fig:s5err}{{(e)}{e}{Subfigure 25e\relax }{subfigure.25.5}{}}
\newlabel{fig:s6err}{{25f}{53}{Subfigure 25f}{subfigure.25.6}{}}
\newlabel{sub@fig:s6err}{{(f)}{f}{Subfigure 25f\relax }{subfigure.25.6}{}}
\newlabel{fig:s7err}{{25g}{53}{Subfigure 25g}{subfigure.25.7}{}}
\newlabel{sub@fig:s7err}{{(g)}{g}{Subfigure 25g\relax }{subfigure.25.7}{}}
\newlabel{fig:s8err}{{25h}{53}{Subfigure 25h}{subfigure.25.8}{}}
\newlabel{sub@fig:s8err}{{(h)}{h}{Subfigure 25h\relax }{subfigure.25.8}{}}
\newlabel{fig:s9err}{{25i}{53}{Subfigure 25i}{subfigure.25.9}{}}
\newlabel{sub@fig:s9err}{{(i)}{i}{Subfigure 25i\relax }{subfigure.25.9}{}}
\newlabel{fig:s9-2err}{{25j}{53}{Subfigure 25j}{subfigure.25.10}{}}
\newlabel{sub@fig:s9-2err}{{(j)}{j}{Subfigure 25j\relax }{subfigure.25.10}{}}
\newlabel{fig:s10err}{{25k}{53}{Subfigure 25k}{subfigure.25.11}{}}
\newlabel{sub@fig:s10err}{{(k)}{k}{Subfigure 25k\relax }{subfigure.25.11}{}}
\newlabel{fig:s11err}{{25l}{53}{Subfigure 25l}{subfigure.25.12}{}}
\newlabel{sub@fig:s11err}{{(l)}{l}{Subfigure 25l\relax }{subfigure.25.12}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {adjusted mean loss\nobreakspace {}=\nobreakspace {}0.0738 std\nobreakspace {}=\nobreakspace {}0.0698.  }}}{53}{subfigure.25.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {adjusted mean loss\nobreakspace {}=\nobreakspace {}0.1932 std\nobreakspace {}=\nobreakspace {}0.2126.  }}}{53}{subfigure.25.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {adjusted mean loss\nobreakspace {}=\nobreakspace {}0.0650 std\nobreakspace {}=\nobreakspace {}0.0902.  }}}{53}{subfigure.25.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {adjusted mean loss\nobreakspace {}=\nobreakspace {}0.0669 std\nobreakspace {}=\nobreakspace {}0.0617.  }}}{53}{subfigure.25.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {adjusted mean loss\nobreakspace {}=\nobreakspace {}0.1214 std\nobreakspace {}=\nobreakspace {}0.1167.  }}}{53}{subfigure.25.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {adjusted mean loss\nobreakspace {}=\nobreakspace {}0.0817 std\nobreakspace {}=\nobreakspace {}0.0895.  }}}{53}{subfigure.25.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {adjusted mean loss\nobreakspace {}=\nobreakspace {}0.0875 std\nobreakspace {}=\nobreakspace {}0.0698.  }}}{53}{subfigure.25.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {adjusted mean loss\nobreakspace {}=\nobreakspace {}0.1932 std\nobreakspace {}=\nobreakspace {}0.2126.  }}}{53}{subfigure.25.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {adjusted mean loss\nobreakspace {}=\nobreakspace {}0.0650 std\nobreakspace {}=\nobreakspace {}0.0902.  }}}{53}{subfigure.25.9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(j)}{\ignorespaces {adjusted mean loss\nobreakspace {}=\nobreakspace {}0.0942 std\nobreakspace {}=\nobreakspace {}0.1061.  }}}{53}{subfigure.25.10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(k)}{\ignorespaces {adjusted mean loss\nobreakspace {}=\nobreakspace {}0.0941 std\nobreakspace {}=\nobreakspace {}0.1085.  }}}{53}{subfigure.25.11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(l)}{\ignorespaces {adjusted mean loss\nobreakspace {}=\nobreakspace {}0.0786 std\nobreakspace {}=\nobreakspace {}0.0693.  }}}{53}{subfigure.25.12}}
\newlabel{fig:s12err}{{24m}{54}{Subfigure 24m}{subfigure.24.13}{}}
\newlabel{sub@fig:s12err}{{(m)}{m}{Subfigure 24m\relax }{subfigure.24.13}{}}
\newlabel{fig:s12-2err}{{24n}{54}{Subfigure 24n}{subfigure.24.14}{}}
\newlabel{sub@fig:s12-2err}{{(n)}{n}{Subfigure 24n\relax }{subfigure.24.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Plots of the loss i.e. the distance between the labelled pose vector and the network's predicted pose vector for the subject fine-tuned models. Every frame with loss greater than $1\sigma $ from the total mean is categorised as a network error (red), an image error (magenta) or a skeleton error (green). Examples of each of these errors are shown in figures \ref  {fig:netErrs}, \ref  {fig:badImages} and \ref  {fig:skelErrs} respectively. The mean for each sequence is recomputed after excluding skeleton and image errors to give a more reliable estimate of the network's true precision. The collective mean after excluding these errors drops from 0.1565 to 0.0874. \relax }}{54}{figure.caption.38}}
\newlabel{fig:classedErr}{{23}{54}{Plots of the loss i.e. the distance between the labelled pose vector and the network's predicted pose vector for the subject fine-tuned models. Every frame with loss greater than $1\sigma $ from the total mean is categorised as a network error (red), an image error (magenta) or a skeleton error (green). Examples of each of these errors are shown in figures \ref {fig:netErrs}, \ref {fig:badImages} and \ref {fig:skelErrs} respectively. The mean for each sequence is recomputed after excluding skeleton and image errors to give a more reliable estimate of the network's true precision. The collective mean after excluding these errors drops from 0.1565 to 0.0874. \relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(m)}{\ignorespaces {adjusted mean loss\nobreakspace {}=\nobreakspace {}0.0815 std\nobreakspace {}=\nobreakspace {}0.0821.  }}}{54}{subfigure.24.13}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(n)}{\ignorespaces {adjusted mean loss\nobreakspace {}=\nobreakspace {}0.0784 std\nobreakspace {}=\nobreakspace {}0.0819.  }}}{54}{subfigure.24.14}}
\newlabel{fig:diffCase1}{{24a}{55}{Subfigure 24a}{subfigure.24.1}{}}
\newlabel{sub@fig:diffCase1}{{(a)}{a}{Subfigure 24a\relax }{subfigure.24.1}{}}
\newlabel{fig:diffCase2}{{24b}{55}{Subfigure 24b}{subfigure.24.2}{}}
\newlabel{sub@fig:diffCase2}{{(b)}{b}{Subfigure 24b\relax }{subfigure.24.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Shows some borderline cases when attributing the causes for errors in figure \ref  {fig:classedErr}. We try to be as objective as possible only ascribing a skeleton error if there is a clear error with the ground truth. These cases are determined to not be clear enough so we assign network errors in both cases.\relax }}{55}{figure.caption.39}}
\newlabel{fig:diffCases}{{24}{55}{Shows some borderline cases when attributing the causes for errors in figure \ref {fig:classedErr}. We try to be as objective as possible only ascribing a skeleton error if there is a clear error with the ground truth. These cases are determined to not be clear enough so we assign network errors in both cases.\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Subject 11 Stop x2 frame 211 loss = 0.3881. A difficult case where the image and network's prediction seem plausible but the skeleton does too. In this case we give the skeleton the benefit of the doubt and assign a network error. }}}{55}{subfigure.24.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Subject 12 Normal 1 frame 256 loss = 0.3511. Here the skeleton says the right knee is behind the left. Image seems to show them at equal depth so we assign a network error.  }}}{55}{subfigure.24.2}}
\citation{Blake2011}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Shows the dynamics quality model (colour) as a function of pose vector and movement stage. The black dots represent frames from the Subject 9 Normal 1 sequence after its division into gait cycles. The strongest indication of normality is the value of Y[0]. Y[2] has a much broader range of normal values.\relax }}{57}{figure.caption.40}}
\newlabel{fig:qualityHeatMap}{{25}{57}{Shows the dynamics quality model (colour) as a function of pose vector and movement stage. The black dots represent frames from the Subject 9 Normal 1 sequence after its division into gait cycles. The strongest indication of normality is the value of Y[0]. Y[2] has a much broader range of normal values.\relax }{figure.caption.40}{}}
\newlabel{fig:lrlom7}{{26a}{58}{Subfigure 26a}{subfigure.26.1}{}}
\newlabel{sub@fig:lrlom7}{{(a)}{a}{Subfigure 26a\relax }{subfigure.26.1}{}}
\newlabel{fig:lrlom10}{{26b}{58}{Subfigure 26b}{subfigure.26.2}{}}
\newlabel{sub@fig:lrlom10}{{(b)}{b}{Subfigure 26b\relax }{subfigure.26.2}{}}
\newlabel{fig:lrlom9}{{26c}{58}{Subfigure 26c}{subfigure.26.3}{}}
\newlabel{sub@fig:lrlom9}{{(c)}{c}{Subfigure 26c\relax }{subfigure.26.3}{}}
\newlabel{fig:lrlom92}{{26d}{58}{Subfigure 26d}{subfigure.26.4}{}}
\newlabel{sub@fig:lrlom92}{{(d)}{d}{Subfigure 26d\relax }{subfigure.26.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Shows the images, label skeletons and their corresponding points on the pose manifold (red) and the networks prediction (blue) for frames in the 3 tested L/RL sequences where both feet are on the same step. In LL sequences we expect the pose vectors to remain in the negative $Y[0]$ region of the manifold and RL sequences the positive. We find the Kinect tends to over measure this phase of the movement which causes these sequences to not always be properly classified as abnormal. In the majority of cases the network measurements do a better job at identifying the abnormality. \relax }}{58}{figure.caption.41}}
\newlabel{fig:lrlOverMeasure}{{26}{58}{Shows the images, label skeletons and their corresponding points on the pose manifold (red) and the networks prediction (blue) for frames in the 3 tested L/RL sequences where both feet are on the same step. In LL sequences we expect the pose vectors to remain in the negative $Y[0]$ region of the manifold and RL sequences the positive. We find the Kinect tends to over measure this phase of the movement which causes these sequences to not always be properly classified as abnormal. In the majority of cases the network measurements do a better job at identifying the abnormality. \relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Subject 7 RL frame 116. Kinect skeleton measures left knee slightly in front of right but image shows to them at equal depth. }}}{58}{subfigure.26.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Subject 10 LL frame 365. Kinect skeleton measures right foot slightly in front of the left.  }}}{58}{subfigure.26.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Subject 9 RL frame 256. In this frame the network measurement is worse than the Kinect's. We think the presence of the subject's arm over the left knee causes the network to think the knee is raised. }}}{58}{subfigure.26.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Subject 9 RL frame 188. In the same sequence when the arm is not covering the knee the network measurement is in line with the other R/LL sequences. }}}{58}{subfigure.26.4}}
\citation{Belagiannis}
\citation{Belagiannis}
\citation{Ionescu2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Processing Time and Memory Requirements.}{59}{subsection.4.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{59}{section.5}}
\newlabel{sec:discussion}{{5}{59}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Data Limitations}{59}{subsection.5.1}}
\citation{Ionescu2014}
\citation{Ionescu2014}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces An example of the depth images contained in the Human3.6M dataset\nobreakspace  {}\cite  {Ionescu2014}. This was produced by a Mesa SwissRanger SR4000 ToF depth sensor. We deemed these images too different from the kind produced by the depth sensors employed in SPHERE to be useful for augmenting training data or assessing system performance. \relax }}{60}{figure.caption.42}}
\newlabel{fig:human36}{{27}{60}{An example of the depth images contained in the Human3.6M dataset~\cite {Ionescu2014}. This was produced by a Mesa SwissRanger SR4000 ToF depth sensor. We deemed these images too different from the kind produced by the depth sensors employed in SPHERE to be useful for augmenting training data or assessing system performance. \relax }{figure.caption.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Measuring accuracy}{60}{subsection.5.2}}
\citation{Tao}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Subject Specific Networks}{61}{subsection.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Shows a regular skeleton in black and a irregular one in red. The pose vectors for these two when projected onto the manifold are plotted on the right. They recieve very similar mappings. This shows that we are not neccessarily guaranteed that pose vectors that are classified as normal actually come from normal skeletons. \relax }}{62}{figure.caption.43}}
\newlabel{fig:abnormalManifold}{{28}{62}{Shows a regular skeleton in black and a irregular one in red. The pose vectors for these two when projected onto the manifold are plotted on the right. They recieve very similar mappings. This shows that we are not neccessarily guaranteed that pose vectors that are classified as normal actually come from normal skeletons. \relax }{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Abnormal Poses}{62}{subsection.5.4}}
\newlabel{sec:abnormals}{{5.4}{62}{Abnormal Poses}{subsection.5.4}{}}
\newlabel{fig:ab1}{{29a}{63}{Subfigure 29a}{subfigure.29.1}{}}
\newlabel{sub@fig:ab1}{{(a)}{a}{Subfigure 29a\relax }{subfigure.29.1}{}}
\newlabel{fig:ab2}{{29b}{63}{Subfigure 29b}{subfigure.29.2}{}}
\newlabel{sub@fig:ab2}{{(b)}{b}{Subfigure 29b\relax }{subfigure.29.2}{}}
\newlabel{fig:ab3}{{29c}{63}{Subfigure 29c}{subfigure.29.3}{}}
\newlabel{sub@fig:ab3}{{(c)}{c}{Subfigure 29c\relax }{subfigure.29.3}{}}
\newlabel{fig:ab4}{{29d}{63}{Subfigure 29d}{subfigure.29.4}{}}
\newlabel{sub@fig:ab4}{{(d)}{d}{Subfigure 29d\relax }{subfigure.29.4}{}}
\newlabel{fig:abPlot}{{29e}{63}{Subfigure 29e}{subfigure.29.5}{}}
\newlabel{sub@fig:abPlot}{{(e)}{e}{Subfigure 29e\relax }{subfigure.29.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces  (a)-(d) shows some examples of the 31 abnormal poses that were tested with the CNN. We find that, given images far from those in the training data, the network predicts outputs clustered around the origin. \relax }}{63}{figure.caption.44}}
\newlabel{fig:abCnn}{{29}{63}{(a)-(d) shows some examples of the 31 abnormal poses that were tested with the CNN. We find that, given images far from those in the training data, the network predicts outputs clustered around the origin. \relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { }}}{63}{subfigure.29.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {  }}}{63}{subfigure.29.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{63}{subfigure.29.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces { }}}{63}{subfigure.29.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces { }}}{63}{subfigure.29.5}}
\citation{Paiement}
\citation{Blake2011}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Directions of Future Work}{64}{subsection.5.5}}
\bibstyle{plain}
\bibdata{library,myLib}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{65}{section.6}}
